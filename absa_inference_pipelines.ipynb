{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c987ed3a-6aee-4658-9269-65cd85252fa9",
   "metadata": {},
   "source": [
    "# Absa Zindi Income Prediction: SageMaker Inference Pipelines\n",
    "\n",
    "This notebook is part of a continuation of the **Absa Zindi Challenge - Income Prediction** project. The initial work on this project aimed to predict customer income based on various features, using advanced machine learning techniques to model the data effectively.\n",
    "\n",
    "In this extended phase, we focus on implementing a **custom Scikit-learn ensemble Random Forest model** instead of XGBoost. The goal is to enhance prediction accuracy and streamline the end-to-end machine learning pipeline using **Amazon SageMaker**. This includes workflows for **model training, hyperparameter optimization**, and **model deployment** to support scalable and robust production environments.\n",
    "\n",
    "### Key Enhancements:\n",
    "1. **Data Processing**: Preprocessing the data to ensure it is clean, well-structured, and optimized for Random Forest modeling.\n",
    "2. **Hyperparameter Optimization (HPO)**: Fine-tuning the Random Forest model’s hyperparameters to achieve the best predictive performance using SageMaker’s HPO capabilities.\n",
    "3. **Model Evaluation**: Assessing model performance using key metrics such as **Root Mean Squared Error (RMSE)**, tailored to evaluate regression tasks effectively.\n",
    "4. **Model Registration**: Storing, versioning, and managing the best-performing Random Forest models with SageMaker Model Registry for easy retrieval and deployment.\n",
    "5. **Model Deployment**: Automating the deployment of the trained model to SageMaker endpoints for real-time inference, as well as batch inference using SageMaker Batch Transform when required.\n",
    "\n",
    "### Project Objectives:\n",
    "- **Improve Prediction Accuracy:** Leverage the strengths of the Scikit-learn ensemble Random Forest to capture complex patterns in customer data.\n",
    "- **Automate the ML Pipeline:** Implement a fully automated workflow covering data preprocessing, model training, evaluation, and deployment.\n",
    "- **Ensure Scalability:** Utilize SageMaker’s infrastructure to handle large datasets efficiently and support both real-time and batch predictions.\n",
    "- **Model Monitoring:** Incorporate model monitoring for continuous performance tracking in production environments.\n",
    "\n",
    "This notebook provides a comprehensive guide on building, optimizing, and deploying machine learning models with Amazon SageMaker, tailored to income prediction based on customer data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601190f7-e665-4df4-9b69-7817aef4ce82",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5997c932-59e8-465f-9d4b-8d0529aa51ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import (\n",
    "    ProcessingInput, \n",
    "    ProcessingOutput, \n",
    "    Processor,\n",
    "    ScriptProcessor\n",
    ")\n",
    "\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker import Model\n",
    "\n",
    "from sagemaker.sklearn.model import SKLearnModel\n",
    "\n",
    "from sagemaker.workflow.steps import (\n",
    "    TuningStep,\n",
    "    ProcessingStep, \n",
    "    CacheConfig,\n",
    "    TransformStep\n",
    ")\n",
    "from sagemaker.tuner import (\n",
    "    ContinuousParameter,\n",
    "    IntegerParameter,\n",
    "    CategoricalParameter,\n",
    "    HyperparameterTuner,\n",
    "    WarmStartConfig,\n",
    "    WarmStartTypes,\n",
    ")\n",
    "from sagemaker.transformer import Transformer\n",
    "from sagemaker.inputs import TransformInput\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "from sagemaker.workflow.conditions import ConditionLessThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.fail_step import FailStep\n",
    "from sagemaker.workflow.functions import Join\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "from sagemaker.model_metrics import (\n",
    "    MetricsSource,\n",
    "    ModelMetrics,\n",
    ")\n",
    "\n",
    "from sagemaker.workflow.functions import Join, JsonGet\n",
    "from sagemaker.workflow.execution_variables import ExecutionVariables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c64956a-0681-4ec6-857d-9739e94909cc",
   "metadata": {},
   "source": [
    "# Some Project Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54911fd3-7dd0-4683-93ed-e57a7cadaed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.session.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "pipeline_session = PipelineSession()\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "model_package_group_name = f\"absa-zindi-model-group\"\n",
    "project_name = \"absa_zindi_challenge\" \n",
    "s3_project_prefix = f\"{project_name}/input-data\"\n",
    "cache_config = CacheConfig(enable_caching=True, expire_after=\"30d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2b1e6e3-359b-4f98-bc3d-c904f7f9c50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sagemaker.session.Session object at 0x7f642c3d3390>\n",
      "us-east-1\n",
      "arn:aws:iam::770208914484:role/service-role/AmazonSageMaker-ExecutionRole-20241229T183387\n",
      "sagemaker-us-east-1-770208914484\n"
     ]
    }
   ],
   "source": [
    "print(sagemaker_session)\n",
    "print(region)\n",
    "print(role)\n",
    "print(default_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1cf731-43aa-49b4-af64-e38ad6159b68",
   "metadata": {},
   "source": [
    "# Input Data s3 location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33f0bb9e-8033-446e-a555-c209b5d2755d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_s3_uri = os.path.join(\"s3://\", default_bucket, s3_project_prefix, \"train\", \"Train.csv\")\n",
    "test_data_s3_uri = os.path.join(\"s3://\", default_bucket, s3_project_prefix, \"test\", \"Test.csv\")\n",
    "customer_data_s3_uri = os.path.join(\"s3://\", default_bucket, s3_project_prefix,\"customer\",  \"customer.csv\")\n",
    "transactions_data_s3_uri = os.path.join(\"s3://\", default_bucket, s3_project_prefix, \"transactions\", \"transactions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5847a66-99ff-4184-967e-e78252bef4f9",
   "metadata": {},
   "source": [
    "# Pipeline Parameters\n",
    "\n",
    "In this section, the code defines various parameters for an Amazon SageMaker pipeline. These parameters allow users to specify values at runtime, such as the instance count, data sources, and thresholds. These parameters are used in different steps of the pipeline to configure the behavior of each step.\n",
    "\n",
    "1. **`ParameterInteger`**: \n",
    "   - **`processing_instance_count`**: Specifies the number of instances to use for the data processing step. It defaults to `1`, but can be modified at runtime.\n",
    "   - **Purpose**: Controls how many instances will be used for the processing tasks like data cleaning or transformation.\n",
    "\n",
    "2. **`ParameterString`**:\n",
    "   - **`instance_type`**: Defines the type of instance for training (e.g., `ml.m4.xlarge`). This can be changed based on available resources or performance requirements.\n",
    "   - **`model_approval_status`**: Specifies the model approval status, which is set to `\"PendingManualApproval\"` by default. This can be modified to `\"Approved\"` or `\"Rejected\"` as needed during model deployment.\n",
    "   - **`train_data`**: The URI of the training data stored in S3. This data is used for training the model.\n",
    "   - **`test_data`**: The URI of the test data stored in S3. This data is used for evaluating the model's performance.\n",
    "   - **`customer_data`**: The URI for the customer-related data in S3.\n",
    "   - **`transactions_data`**: The URI for transaction-related data stored in S3.\n",
    "   \n",
    "3. **`ParameterFloat`**:\n",
    "   - **`rmse_score_threshold`**: A floating-point parameter specifying the threshold for the RMSE (Root Mean Squared Error). If the model's RMSE score is greater than this threshold, it may trigger certain actions like re-training or rejection.\n",
    "\n",
    "#### Purpose of Each Parameter:\n",
    "- **`ParameterInteger`**: For integer values like instance counts.\n",
    "- **`ParameterString`**: For string values like URIs, model approval status, and instance types.\n",
    "- **`ParameterFloat`**: For numerical values with decimal points, such as thresholds for performance metrics like RMSE.\n",
    "\n",
    "#### Use Case:\n",
    "These parameters are critical when running the SageMaker pipeline because they allow users to define the behavior of different pipeline steps without modifying the code itself. Instead, they can modify values such as the data sources or instance types at runtime, making the pipeline more flexible and reusable.\n",
    "\n",
    "#### Example:\n",
    "- **Train Data Parameter**: By setting the `train_data` parameter, users can specify which S3 location to use for training without changing the code.\n",
    "- **RMSE Threshold Parameter**: If users want to adjust the model's performance expectations, they can update the `rmse_score_threshold` parameter to set a different threshold for RMSE values that trigger further steps.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a165bef-c4e6-449d-9970-1495b37c0a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    "    ParameterFloat,\n",
    ")\n",
    "\n",
    "processing_instance_count = ParameterInteger(name=\"ProcessingInstanceCount\", default_value=1)\n",
    "instance_type = ParameterString(name=\"TrainingInstanceType\", default_value='ml.m4.xlarge')\n",
    "\n",
    "model_approval_status = ParameterString(\n",
    "    name=\"ModelApprovalStatus\", default_value=\"PendingManualApproval\"\n",
    ")\n",
    "\n",
    "train_data = ParameterString(\n",
    "    name=\"TrainData\",\n",
    "    default_value=train_data_s3_uri\n",
    ")\n",
    "\n",
    "test_data = ParameterString(\n",
    "    name=\"TestData\",\n",
    "    default_value=test_data_s3_uri\n",
    ")\n",
    "\n",
    "customer_data = ParameterString(\n",
    "    name=\"CustomerData\",\n",
    "    default_value=customer_data_s3_uri\n",
    ")\n",
    "\n",
    "transactions_data = ParameterString(\n",
    "    name=\"TransactionsData\",\n",
    "    default_value=transactions_data_s3_uri\n",
    ")\n",
    "\n",
    "rmse_score_threshold = ParameterFloat(name=\"RMSEThreshold\", default_value=6500.00)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef1708e-964f-4b00-889f-2339fc5b9dbc",
   "metadata": {},
   "source": [
    "# Preprocessing Step\n",
    "\n",
    "## SageMaker Preprocessing Script Overview\n",
    "\n",
    "This script prepares data for a machine learning model in Amazon SageMaker, focusing on feature engineering, data merging, and dataset splitting.\n",
    "\n",
    "#### Imports and Logging\n",
    "- **Libraries:** `pandas`, `numpy`, `sklearn`, `joblib`, `argparse`, `os`, `logging`\n",
    "- **Logging:** Tracks script execution with `INFO` level logs.\n",
    "\n",
    "#### Helper Functions\n",
    "\n",
    "- **`flatten_transactions(df)`**\n",
    "  - Creates transaction-related features:\n",
    "    - Flags for transaction types (`IS_CHEQ_TRANS`, `IS_SAVE_TRANS`).\n",
    "    - Amount calculations (`CHEQ_AMT`, `SAVE_AMT`, `INBOUND_AMT`, `OUTBOUND_AMT`).\n",
    "    - Channel flags (`IS_SYSTEM_CHANNEL`, `IS_ATM_CHANNEL`, etc.).\n",
    "    - Transaction descriptions (`IS_POS_TRANS`, `IS_ATM_WITHDRAW_TRANS`, etc.).\n",
    "  - Aggregates data by `CUSTOMER_IDENTIFIER` and `RECORD_MONTH`:\n",
    "    - Counts transactions, calculates mean, max, min, median, standard deviation, and 90th percentile for balances and transaction amounts.\n",
    "  - Further aggregates at the customer level using:\n",
    "    - Mean, min, max, median, standard deviation, and 75th percentile.\n",
    "\n",
    "- **`remove_correlated_features(df, threshold=0.9)`**\n",
    "  - Identifies highly correlated features using the correlation matrix.\n",
    "  - Removes one feature from each pair with a correlation above 0.9 to reduce multicollinearity.\n",
    "\n",
    "- **`merge_and_flatted_trans_agg(df)`**\n",
    "  - Merges the input dataframe with:\n",
    "    - `customer_df` (customer-level information).\n",
    "    - `transactions_df` (flattened transaction features).\n",
    "  - Fills missing values with zeros and sets the index using `CUSTOMER_IDENTIFIER`, `RECORD_DATE`, and `DATE_LAST_UPDATED`.\n",
    "\n",
    "#### Main Script Logic\n",
    "\n",
    "1. **Argument Parsing:**\n",
    "   - Accepts `--train-size` (default: 0.8) and `--random-state` (default: 0) for data splitting control.\n",
    "\n",
    "2. **Data Loading:**\n",
    "   - Reads input data from `/opt/ml/processing/input`:\n",
    "     - `Train.csv`, `Test.csv`, `customer.csv`, `transactions.csv`.\n",
    "   - Processes `transactions_df`:\n",
    "     - Converts `RECORD_DATE` to datetime and extracts `RECORD_MONTH`.\n",
    "     - Applies `flatten_transactions()` and `remove_correlated_features()`.\n",
    "\n",
    "3. **Data Merging:**\n",
    "   - Merges `train_df` and `test_df` with customer and transaction data using `merge_and_flatted_trans_agg()`.\n",
    "\n",
    "4. **Train-Test Split:**\n",
    "   - Splits `train_df` into training (80%) and testing (20%) datasets.\n",
    "   - Further splits the training set into training (85%) and validation (15%) subsets.\n",
    "\n",
    "5. **Feature Selection with RFE (Recursive Feature Elimination):**\n",
    "   - Uses `RandomForestRegressor` with `RFECV` for feature selection.\n",
    "   - Selects features based on 5-fold cross-validation using negative RMSE as the scoring metric.\n",
    "\n",
    "6. **Saving Processed Data:**\n",
    "   - Writes processed datasets to `/opt/ml/processing/output`:\n",
    "     - Training, validation, testing datasets in CSV format (without headers and indices).\n",
    "     - A `to_be_submitted.csv` file for submission purposes.\n",
    "\n",
    "7. **Logging Final Information:**\n",
    "   - Logs the remaining columns after feature selection.\n",
    "   - Logs the sizes of the training, validation, test, and submission datasets.\n",
    "\n",
    "#### Key Outputs\n",
    "- **Processed Datasets:**\n",
    "  - `train.csv`, `validation.csv`, `test.csv`, `to_be_submitted.csv`\n",
    "- **Logs:**\n",
    "  - Feature selection details, dataset sizes, and confirmation of script completion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "facb0222-7d98-43b6-a69b-028feac56617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/preprocessing.py\n",
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import joblib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "def flatten_transactions(df):\n",
    "    # creating helper columns\n",
    "    df[\"IS_CHEQ_TRANS\"] = np.where(df.PRODUCT_CODE == \"CHEQ\", 1, 0)\n",
    "    df[\"IS_SAVE_TRANS\"] = np.where(df.PRODUCT_CODE == \"SAVE\", 1, 0)\n",
    "    df[\"CHEQ_AMT\"] = df.loc[df.PRODUCT_CODE == \"CHEQ\", \"AMT\"]\n",
    "    df[\"SAVE_AMT\"] = df.loc[df.PRODUCT_CODE == \"SAVE\", \"AMT\"]\n",
    "    df[\"INBOUND_AMT\"] = df.loc[df.AMT>0, \"AMT\"]\n",
    "    df[\"OUTBOUND_AMT\"] = df.loc[df.AMT<0, \"AMT\"]\n",
    "    df[\"CHEQ_INBOUND_AMT\"] = df.loc[(df.PRODUCT_CODE == \"CHEQ\") & (df.AMT>0), \"AMT\"]\n",
    "    df[\"SAVE_INBOUND_AMT\"] = df.loc[(df.PRODUCT_CODE == \"SAVE\") & (df.AMT>0), \"AMT\"]\n",
    "    df[\"CHEQ_OUTBOUND_AMT\"] = np.abs(df.loc[(df.PRODUCT_CODE == \"CHEQ\") & (df.AMT<0), \"AMT\"])\n",
    "    df[\"SAVE_OUTBOUND_AMT\"] = np.abs(df.loc[(df.PRODUCT_CODE == \"SAVE\") & (df.AMT<0), \"AMT\"])\n",
    "    \n",
    "    df[\"IS_SYSTEM_CHANNEL\"] = np.where(df.CHANNEL == \"system\", 1, 0)\n",
    "    df[\"IS_ATM_CHANNEL\"] = np.where(df.CHANNEL == \"atm\", 1, 0)\n",
    "    df[\"IS_INTERNET_CHANNEL\"] = np.where(df.CHANNEL == \"internet\", 1, 0)\n",
    "    df[\"IS_TELLER_CHANNEL\"] = np.where(df.CHANNEL == \"teller\", 1, 0)\n",
    "    \n",
    "    df[\"CHEQ_ACCOUNT_BALANCE\"] = df.loc[df.PRODUCT_CODE == \"CHEQ\", \"ACCOUNT_BALANCE\"]\n",
    "    df[\"SAVE_ACCOUNT_BALANCE\"] = df.loc[df.PRODUCT_CODE == \"SAVE\", \"ACCOUNT_BALANCE\"]\n",
    "    \n",
    "    df[\"IS_POS_TRANS\"] =np.where(df.TRANSACTION_DESCRIPTION == \"POS PURCHASE\",1,0)\n",
    "    df[\"IS_ATM_WITHDRAW_TRANS\"] =np.where(df.TRANSACTION_DESCRIPTION == \"ATM WITHDRAWAL\",1,0)\n",
    "    df[\"IS_AIRTIME_DEBIT_TRANS\"] =np.where(df.TRANSACTION_DESCRIPTION == \"AIRTIME DEBIT\",1,0)\n",
    "\n",
    "    trans_per_cust_per_month_df = (\n",
    "        df\n",
    "        .groupby([\"CUSTOMER_IDENTIFIER\", \"RECORD_MONTH\"])\n",
    "        .agg(CNT_TRNS = (\"EVENT_NUMBER\", \"count\"),\n",
    "             AVG_ACC_BAL = (\"ACCOUNT_BALANCE\", \"mean\"),\n",
    "             MAX_ACC_BAL =  (\"ACCOUNT_BALANCE\", \"max\"),\n",
    "             MIN_ACC_BAL =  (\"ACCOUNT_BALANCE\", \"min\"),\n",
    "             MEDIAN_ACC_BAL =  (\"ACCOUNT_BALANCE\", \"median\"),\n",
    "             STDDEV_ACC_BAL =  (\"ACCOUNT_BALANCE\", \"std\"),\n",
    "             NINETIENT_ACC_BAL = ('ACCOUNT_BALANCE', lambda x: x.quantile(0.9)),\n",
    "             AVG_INBOUND_AMT = (\"INBOUND_AMT\", \"mean\"),\n",
    "             MAX_INBOUND_AMT =  (\"INBOUND_AMT\", \"max\"),\n",
    "             MIN_INBOUND_AMT =  (\"INBOUND_AMT\", \"min\"),\n",
    "             MEDIAN_INBOUND_AMT =  (\"INBOUND_AMT\", \"median\"),\n",
    "             STDDEV_INBOUND_AMT =  (\"INBOUND_AMT\", \"std\"),\n",
    "             NINETIENT_INBOUND_AMT = (\"INBOUND_AMT\", lambda x: x.quantile(0.9)),\n",
    "             TOTAL_INBOUND_AMT = (\"INBOUND_AMT\", \"sum\"),\n",
    "             AVG_OUTBOUND_AMT = (\"OUTBOUND_AMT\", \"mean\"),\n",
    "             MAX_OUTBOUND_AMT =  (\"OUTBOUND_AMT\", \"max\"),\n",
    "             MIN_OUTBOUND_AMT =  (\"OUTBOUND_AMT\", \"min\"),\n",
    "             MEDIAN_OUTBOUND_AMT =  (\"OUTBOUND_AMT\", \"median\"),\n",
    "             STDDEV_OUTBOUND_AMT =  (\"OUTBOUND_AMT\", \"std\"),\n",
    "             NINETIENT_OUTBOUND_AMT = (\"OUTBOUND_AMT\", lambda x: x.quantile(0.9)),\n",
    "             TOTAL_OUTBOUND_AMT = (\"OUTBOUND_AMT\", \"sum\"),\n",
    "             CNT_CHEQ_TRANS = (\"IS_CHEQ_TRANS\", \"sum\"),\n",
    "             CNT_SAVE_TRANS = (\"IS_SAVE_TRANS\", \"sum\"),\n",
    "             CHEQ_INBOUND_AMT  = (\"CHEQ_INBOUND_AMT\", \"sum\"),\n",
    "             SAVE_INBOUND_AMT  = (\"SAVE_INBOUND_AMT\", \"sum\"),\n",
    "             CHEQ_OUTBOUND_AMT  = (\"CHEQ_OUTBOUND_AMT\", \"sum\"),\n",
    "             SAVE_OUTBOUND_AMT  = (\"SAVE_OUTBOUND_AMT\", \"sum\"),\n",
    "             CNT_SYS_TRNS = (\"IS_SYSTEM_CHANNEL\", \"sum\"),\n",
    "             CNT_ATM_TRNS = (\"IS_ATM_CHANNEL\", \"sum\"),\n",
    "             CNT_INTERNET_TRNS  = (\"IS_INTERNET_CHANNEL\", \"sum\"),\n",
    "             CNT_TELLER_TRNS = (\"IS_TELLER_CHANNEL\", \"sum\"),\n",
    "             AVG_CHEQ_ACC_BAL  = (\"CHEQ_ACCOUNT_BALANCE\", \"mean\"),\n",
    "             AVG_SAVE_ACC_BAL  = (\"SAVE_ACCOUNT_BALANCE\", \"mean\"),\n",
    "             MAX_CHEQ_ACC_BAL  = (\"CHEQ_ACCOUNT_BALANCE\", \"max\"),\n",
    "             MAX_SAVE_ACC_BAL  = (\"SAVE_ACCOUNT_BALANCE\", \"max\"),\n",
    "             MEDIAN_CHEQ_ACC_BAL  = (\"CHEQ_ACCOUNT_BALANCE\", \"median\"),\n",
    "             MEDIAN_SAVE_ACC_BAL  = (\"SAVE_ACCOUNT_BALANCE\", \"median\"),\n",
    "             CNT_POS_TRNS  = (\"IS_POS_TRANS\", \"sum\"),\n",
    "             CNT_ATM_WITHDRAW_TRNS  = (\"IS_ATM_WITHDRAW_TRANS\", \"sum\"),\n",
    "             CNT_AIRTIME_DEBIT_TRNS  = (\"IS_AIRTIME_DEBIT_TRANS\", \"sum\"),\n",
    "             \n",
    "            )\n",
    "    ).reset_index()\n",
    "    \n",
    "    agg_funcs = ['mean', 'min', 'max', 'median', 'std', lambda x: x.quantile(0.75)]\n",
    "    \n",
    "    # Perform the groupby operation\n",
    "    flat_trans_df = (\n",
    "        trans_per_cust_per_month_df\n",
    "        .groupby('CUSTOMER_IDENTIFIER')\n",
    "        [trans_per_cust_per_month_df.columns[2:]]\n",
    "        .agg(agg_funcs)\n",
    "    )\n",
    "    # Rename the last column (75th percentile)\n",
    "    flat_trans_df.columns = ['_'.join(col).upper() if isinstance(col, tuple) else col for col in flat_trans_df.columns]\n",
    "    flat_trans_df = flat_trans_df.rename(\n",
    "        columns={col: col.replace('<LAMBDA_0>', '75TH_PERC') for col in flat_trans_df.columns}\n",
    "    ).reset_index()\n",
    "    return flat_trans_df\n",
    "\n",
    "def remove_correlated_features(df, threshold=0.9):\n",
    "    corr_matrix = df.iloc[:, 1:].corr().abs()  # Use absolute values to check strength of correlation\n",
    "    # Identify highly correlated features\n",
    "    to_drop = set()  # Set to store columns to drop\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i + 1, len(corr_matrix.columns)):  # Avoid diagonal and duplicate pairs\n",
    "            if corr_matrix.iloc[i, j] > threshold:\n",
    "                colname = corr_matrix.columns[j]  # Select the second variable in the pair\n",
    "                to_drop.add(colname)\n",
    "    logger.info(f\"Highly correlated features removed {to_drop}\")\n",
    "    return df.drop(columns=to_drop)\n",
    "    \n",
    "def merge_and_flatted_trans_agg(df):\n",
    "    out_df = df.merge(customer_df, on='CUSTOMER_IDENTIFIER')\n",
    "    out_df = out_df.merge(transactions_df, on=\"CUSTOMER_IDENTIFIER\")\n",
    "    return out_df.set_index([\"CUSTOMER_IDENTIFIER\", \"RECORD_DATE\", \"DATE_LAST_UPDATED\"]).fillna(0)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    logger.info(\"Loading arguments\")\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--train-size\", type=float, default=0.8)\n",
    "    parser.add_argument(\"--random-state\", type=int, default=0)\n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    base_dir = \"/opt/ml/processing\"\n",
    "    train_df = pd.read_csv(os.path.join(base_dir, \"input\",\"train\", \"Train.csv\"), thousands=r',')\n",
    "    test_df = pd.read_csv(os.path.join(base_dir, \"input\",\"test\", \"Test.csv\"))\n",
    "    customer_df = pd.read_csv(os.path.join(base_dir, \"input\",\"customer\", \"customer.csv\"))\n",
    "    transactions_df = pd.read_csv(os.path.join(base_dir, \"input\",\"transactions\", \"transactions.csv\"))\n",
    "    transactions_df['RECORD_DATE'] = pd.to_datetime(transactions_df['RECORD_DATE'])\n",
    "    transactions_df['RECORD_MONTH']  = transactions_df['RECORD_DATE'].dt.to_period('M')\n",
    "    transactions_df = remove_correlated_features(flatten_transactions(transactions_df))\n",
    "    \n",
    "    train_df = merge_and_flatted_trans_agg(train_df)\n",
    "    to_be_submitted_df = merge_and_flatted_trans_agg(test_df)\n",
    "    \n",
    "    train, test = train_test_split(train_df, train_size=float(args.train_size), random_state=int(args.random_state))\n",
    "    train, val = train_test_split(train, train_size=0.85, random_state=int(args.random_state))\n",
    "\n",
    "    y = train[\"DECLARED_NET_INCOME\"]\n",
    "    X = train.iloc[:,1:]\n",
    "    logger.info(\"Initiating RFE...\")\n",
    "    rf = RandomForestRegressor(random_state=345)\n",
    "    selector = RFECV(rf, step=1, cv=5, scoring=\"neg_root_mean_squared_error\")\n",
    "    selector = selector.fit(X, y)\n",
    "    train = pd.concat([y, X.loc[:,selector.support_]], axis=1)\n",
    "    remaining_cols = list(train.columns)\n",
    "    logger.info(f\"columns after RFE {remaining_cols}\")\n",
    "    logger.info(f\"Train size : {train.shape[0]}\")\n",
    "    logger.info(f\"Validation size : {val.shape[0]}\")\n",
    "    logger.info(f\"Test size : {test.shape[0]}\")\n",
    "    logger.info(f\"Tp be submitted size : {len(to_be_submitted_df)}\")\n",
    "    pd.DataFrame(train).to_csv(\n",
    "        f\"{base_dir}/output/train/train.csv\",\n",
    "        header=False, \n",
    "        index=False\n",
    "    )\n",
    "    pd.DataFrame(val)[remaining_cols].to_csv(\n",
    "        f\"{base_dir}/output/validation/validation.csv\",  \n",
    "        header=False, \n",
    "        index=False\n",
    "    )\n",
    "    pd.DataFrame(test)[remaining_cols].to_csv(\n",
    "        f\"{base_dir}/output/test/test.csv\",  \n",
    "        header=False, \n",
    "        index=False\n",
    "    )\n",
    "    remaining_cols.remove(\"DECLARED_NET_INCOME\")\n",
    "    to_be_submitted_df[remaining_cols].to_csv(\n",
    "        f\"{base_dir}/output/to_be_submitted/to_be_submitted.csv\",  \n",
    "        # header=False,\n",
    "        # index=False\n",
    "    )\n",
    "    logger.info(\"Processing job complete!\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7e203f-bbd4-49db-973d-f12e3dfd4af1",
   "metadata": {},
   "source": [
    "### SageMaker SKLearnProcessor Configuration\n",
    "\n",
    "This configuration sets up a SageMaker `SKLearnProcessor` for data preprocessing, a critical step in any machine learning pipeline. Preprocessing ensures that the raw data is clean, well-structured, and ready for model training, directly impacting the model's accuracy and performance.\n",
    "\n",
    "#### Processor\n",
    "---------------\n",
    "The `SKLearnProcessor` is initialized with the following parameters:\n",
    "- **Framework Version:** `1.2-1` – Ensures compatibility with specific scikit-learn features.\n",
    "- **Instance Type:** `ml.t3.large` – Balances cost and performance for moderate data processing tasks.\n",
    "- **Instance Count:** `processing_instance_count` – Scales processing based on workload.\n",
    "- **Base Job Name:** `\"sklearn-absa-zindi-process\"` – Provides consistent job naming for easier tracking.\n",
    "- **Role:** `role` – Grants the necessary permissions for SageMaker to access resources securely.\n",
    "- **Session:** `pipeline_session` – Integrates with SageMaker Pipelines for streamlined workflows.\n",
    "\n",
    "#### Inputs\n",
    "-------------\n",
    "\n",
    "Data inputs are essential for feeding raw datasets into the processing job:\n",
    "- **Train Data:** `/opt/ml/processing/input/train` – The primary dataset for model learning.\n",
    "- **Test Data:** `/opt/ml/processing/input/test` – Used to evaluate model performance post-training.\n",
    "- **Transactions Data:** `/opt/ml/processing/input/transactions` – Adds transactional context to the model.\n",
    "- **Customer Data:** `/opt/ml/processing/input/customer` – Enhances feature engineering with customer details.\n",
    "\n",
    "#### Outputs\n",
    "------------\n",
    "Processed outputs are stored in Amazon S3 for downstream tasks:\n",
    "- **Train Data:** Preprocessed data for model training.\n",
    "- **Validation Data:** Used to fine-tune model hyperparameters.\n",
    "- **Test Data:** Final dataset to assess model generalization.\n",
    "- **Submission File:** Data formatted for competition or business submission requirements.\n",
    "- **Artifacts:** Stores metadata, logs, or other relevant processing artifacts.\n",
    "\n",
    "#### Processing Script\n",
    "-----------------\n",
    "- **Script:** `code/preprocessing.py` – Automates data cleaning, feature engineering, and transformation,\n",
    "- **Arguments:**\n",
    "  - `--train-size 0.9` – Allocates 90% of data for training, optimizing learning while leaving enough for validation.\n",
    "  - `--random_state 100` – Ensures reproducibility of results for consistent model evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3cd7c61-f818-4bda-ac9f-8ac941f8e788",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sagemaker/workflow/pipeline_context.py:332: UserWarning: Running within a PipelineSession, there will be No Wait, No Logs, and No Job being started.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "framework_version = \"1.2-1\"\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=framework_version,\n",
    "    instance_type='ml.t3.large',\n",
    "    instance_count=processing_instance_count,\n",
    "    base_job_name=\"sklearn-absa-zindi-process\",\n",
    "    role=role,\n",
    "    sagemaker_session=pipeline_session,\n",
    ")\n",
    "\n",
    "processor_args = sklearn_processor.run(\n",
    "    inputs=[\n",
    "        ProcessingInput(source=train_data_s3_uri, destination=\"/opt/ml/processing/input/train\"),\n",
    "        ProcessingInput(source=test_data_s3_uri, destination=\"/opt/ml/processing/input/test\"),\n",
    "        ProcessingInput(source=transactions_data_s3_uri, destination=\"/opt/ml/processing/input/transactions\"),\n",
    "        ProcessingInput(source=customer_data_s3_uri, destination=\"/opt/ml/processing/input/customer\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"train\",\n",
    "            source=\"/opt/ml/processing/output/train\",\n",
    "            destination = f\"s3://{default_bucket}/{project_name}/opt/ml/processing/output/train\"\n",
    "                        ),\n",
    "        \n",
    "        ProcessingOutput(\n",
    "            output_name=\"validation\",\n",
    "            source=\"/opt/ml/processing/output/validation\",\n",
    "            destination = f\"s3://{default_bucket}/{project_name}/opt/ml/processing/output/validation\"),\n",
    "        ProcessingOutput(\n",
    "            output_name=\"test\",\n",
    "            source=\"/opt/ml/processing/output/test\",\n",
    "            destination = f\"s3://{default_bucket}/{project_name}/opt/ml/processing/output/test\"\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name=\"to_be_submitted\",\n",
    "            source=\"/opt/ml/processing/output/to_be_submitted\",\n",
    "            destination = f\"s3://{default_bucket}/{project_name}/opt/ml/processing/output/to_be_submitted\"\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name=\"artifacts\",\n",
    "            source=\"/opt/ml/processing/output/artifacts\",\n",
    "            destination = f\"s3://{default_bucket}/{project_name}/opt/ml/processing/output/artifacts\"\n",
    "        )\n",
    "    ],\n",
    "    code=\"code/preprocessing.py\",\n",
    "    arguments=[\n",
    "        \"--train-size\" ,'0.9',\n",
    "        \"--random_state\", '100'])\n",
    "step_process = ProcessingStep(name=\"AbsaZindiProcess\", step_args=processor_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57774ad-832f-4084-ae95-90c1cb3ca6a6",
   "metadata": {},
   "source": [
    "# Tuning Step\n",
    "\n",
    "### Code Explanation: `train.py`\n",
    "\n",
    "#### Imports and Setup\n",
    "-------------------------------\n",
    "   The script begins by importing necessary libraries:\n",
    "   - `argparse`: To handle command-line arguments.\n",
    "   - `joblib`: For saving and loading the trained model.\n",
    "   - `os`: To interact with the operating system (e.g., file paths).\n",
    "   - `logging`: To record the progress and status of the script.\n",
    "   - `numpy` and `pandas`: For numerical and data manipulation.\n",
    "   - `RandomForestRegressor` and `mean_squared_error` from `sklearn`: For building and evaluating a machine learning model (Random Forest Regressor).\n",
    "\n",
    "   The logger is set up to output information to the console.\n",
    "\n",
    "#### Argument Parsing\n",
    "-------\n",
    "   The script uses `argparse` to parse several command-line arguments:\n",
    "   - `--model-dir`: Directory to save the trained model.\n",
    "   - `--train`: Directory for the training data.\n",
    "   - `--validation`: Directory for the validation data.\n",
    "   - `--n_estimators`: Number of trees in the Random Forest.\n",
    "   - `--max_depth`: Maximum depth of the trees.\n",
    "   - `--min_samples_split`: Minimum number of samples required to split a node.\n",
    "   - `--min_samples_leaf`: Minimum number of samples required in a leaf.\n",
    "   - `--max_features`: Number of features to consider when looking for the best split.\n",
    "\n",
    "   These arguments can either be passed through the command line or be retrieved from environment variables if not provided explicitly.\n",
    "   \n",
    "#### Loading Data\n",
    "---------\n",
    "   The script logs the start of the data extraction process. It then loads the training and validation datasets using `pandas.read_csv()`. The CSV files are expected to be in the directories provided by the `--train` and `--validation` arguments.\n",
    "\n",
    "   - `train_df`: Training data (features and target values).\n",
    "   - `validation_df`: Validation data (features and target values).\n",
    "\n",
    "   The `header=None` parameter assumes that the datasets do not have headers.\n",
    "\n",
    "#### Preprocessing Data\n",
    "-------\n",
    "   - The features (`X_train` and `X_val`) are extracted from the datasets, which are all columns except the first one (the target variable).\n",
    "   - The target variable (`y_train` and `y_val`) is the first column of each dataset.\n",
    "\n",
    "#### Training the Model\n",
    "------\n",
    "   The script initializes a `RandomForestRegressor` model with the parameters provided through the command line or environment variables. The model is then trained using the `fit()` method on the training data (`X_train`, `y_train`).\n",
    "\n",
    "   After training, predictions are made on both the training and validation data using the `predict()` method. The Root Mean Squared Error (RMSE) is calculated for both training and validation predictions using `mean_squared_error()` and logged.\n",
    "\n",
    "#### Persisting the Model\n",
    "--------\n",
    "   The trained model is saved using `joblib.dump()` to the path specified by the `--model-dir` argument. This allows the model to be reused in the future without retraining.\n",
    "\n",
    "#### Logging\n",
    "-------\n",
    "   Throughout the script, logging statements are used to provide insights into each step, including:\n",
    "   - Extracting arguments.\n",
    "   - Reading data.\n",
    "   - Building datasets.\n",
    "   - Training the model.\n",
    "   - Reporting evaluation metrics.\n",
    "   - Persisting the trained model.\n",
    "\n",
    "#### Key Functions:\n",
    "------\n",
    "- **`argparse.ArgumentParser()`**: For command-line argument parsing.\n",
    "- **`RandomForestRegressor.fit()`**: To train the model on the data.\n",
    "- **`RandomForestRegressor.predict()`**: To generate predictions from the trained model.\n",
    "- **`mean_squared_error()`**: To calculate the RMSE of the predictions.\n",
    "- **`joblib.dump()`**: To save the trained model to disk.\n",
    "\n",
    "#### Example of Usage (Local mode):\n",
    "-----\n",
    "To run the script with the necessary arguments, you can use the following command (assuming the script is saved as `train.py`):\n",
    "```bash\n",
    "python train.py --model-dir ./models --train ./data/train --validation ./data/validation --n_estimators 200 --max_depth 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4913a1c-2d85-4bff-b77a-da049d5c691f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/train.py\n",
    "import argparse\n",
    "import joblib\n",
    "import os\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"extracting arguments...\")\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Data, model, and output directories\n",
    "    parser.add_argument(\"--model-dir\", type=str, default=os.environ.get(\"SM_MODEL_DIR\"))\n",
    "    parser.add_argument(\"--train\", type=str, default=os.environ.get(\"SM_CHANNEL_TRAIN\"))\n",
    "    parser.add_argument(\"--validation\", type=str, default=os.environ.get(\"SM_CHANNEL_VALIDATION\"))\n",
    "    parser.add_argument(\"--n_estimators\", type=int, default=100)  # Number of trees\n",
    "    parser.add_argument(\"--max_depth\", type=int, default=None)\n",
    "    parser.add_argument(\"--min_samples_split\", type=int, default=2)  # Minimum samples to split a node\n",
    "    parser.add_argument(\"--min_samples_leaf\", type=int, default=1)  # Minimum samples in a leaf\n",
    "    parser.add_argument(\"--max_features\", type=str, default=\"auto\")  # Number of features per split\n",
    "    \n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    logger.info(\"reading data...\")\n",
    "    train_df = pd.read_csv(os.path.join(args.train, \"train.csv\"), header=None)\n",
    "    validation_df = pd.read_csv(os.path.join(args.validation, \"validation.csv\"), header=None)\n",
    "    \n",
    "    logger.info(\"building training and testing datasets...\")                      \n",
    "    X_train = train_df.values[:,1:]\n",
    "    X_val = validation_df.values[:,1:]\n",
    "    y_train = train_df.values[:,0] \n",
    "    y_val = validation_df.values[:,0]\n",
    "\n",
    "    # train\n",
    "    logger.info(\"training model...\")\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=args.n_estimators,\n",
    "        max_depth=args.max_depth,\n",
    "        min_samples_split=args.min_samples_split, \n",
    "        min_samples_leaf=args.min_samples_leaf,\n",
    "        max_features=args.max_features,\n",
    "        random_state=12\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_val_pred =  model.predict(X_val)\n",
    "    \n",
    "    logger.info(f\"train:rmse {np.sqrt(mean_squared_error(y_train, y_train_pred))}\")\n",
    "    logger.info(f\"validation:rmse {np.sqrt(mean_squared_error(y_val, y_val_pred))}\")\n",
    "\n",
    "    # persist model\n",
    "    path = os.path.join(args.model_dir, \"model.joblib\")\n",
    "    joblib.dump(model, path)\n",
    "    logger.info(\"model persisted at \" + path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90536a7a-ff21-402c-b197-20515bfde0e0",
   "metadata": {},
   "source": [
    "### Sagemaker Tuning Job Setup\n",
    "\n",
    "This code performs a hyperparameter tuning job for a machine learning model using Amazon SageMaker. Here's what each part does:\n",
    "\n",
    "#### Model Output Path Setup:\n",
    "   - `model_prefix` defines a prefix for storing model artifacts.\n",
    "   - `model_path` specifies the S3 path to store model artifacts in the bucket.\n",
    "\n",
    "#### SKLearn Estimator Setup:\n",
    "   - `sklearn_estimator` initializes an `SKLearn` estimator object, which is used to run a scikit-learn model training job. The parameters include:\n",
    "     - `entry_point`: Path to the script (`train.py`) that contains the training code.\n",
    "     - `output_dir` and `output_path`: Directories for saving the model artifacts.\n",
    "     - `role`: IAM role for SageMaker to use.\n",
    "     - `instance_count` and `instance_type`: Specify the number of instances and the instance type for training.\n",
    "     - `framework_version`: Version of scikit-learn being used.\n",
    "     - `base_job_name`: Base name for the training job.\n",
    "     - `sagemaker_session`: Session object used to interact with SageMaker.\n",
    "\n",
    "#### Metric Definitions:\n",
    "   - Defines the metrics (`validation:rmse` and `train:rmse`) for monitoring the performance of the model during training and validation. These metrics are captured using regular expressions.\n",
    "\n",
    "\n",
    "#### Hyperparameter Tuning Parameters\n",
    "\n",
    "In the provided code, the hyperparameters being tuned are used to optimize the performance of a machine learning model. Here's an expanded breakdown of the specific parameters:\n",
    "\n",
    "**`n_estimators`**:\n",
    "   - **Type**: `IntegerParameter`\n",
    "   - **Range**: 50 to 3500\n",
    "   - **Description**: This parameter specifies the number of trees in an ensemble model (e.g., Random Forest, Gradient Boosting). A higher number of trees can improve the model's performance but may also lead to increased computational cost. The tuning range is designed to explore different tree counts to find the optimal balance between performance and computation time.\n",
    "\n",
    "**`max_depth`**:\n",
    "   - **Type**: `IntegerParameter`\n",
    "   - **Range**: 3 to 50\n",
    "   - **Description**: This controls the maximum depth of each individual tree. A smaller depth prevents the model from overfitting by limiting its complexity, while a larger depth may allow the model to capture more intricate patterns, but it can also increase the risk of overfitting. Tuning `max_depth` helps find the optimal tree depth for generalization.\n",
    "\n",
    "**`min_samples_split`**:\n",
    "   - **Type**: `IntegerParameter`\n",
    "   - **Range**: 2 to 50\n",
    "   - **Description**: This parameter determines the minimum number of samples required to split an internal node. A smaller value allows the tree to learn finer patterns, while a higher value forces the tree to make broader splits, which can help avoid overfitting. By tuning this, the model can learn the most effective splitting criteria.\n",
    "\n",
    "**`min_samples_leaf`**:\n",
    "   - **Type**: `IntegerParameter`\n",
    "   - **Range**: 1 to 40\n",
    "   - **Description**: The minimum number of samples required to be at a leaf node. Lower values lead to more complex models as each leaf node can have fewer samples, which might increase overfitting. Larger values ensure more data points in each leaf, which can lead to a simpler model and better generalization.\n",
    "\n",
    "**`max_features`**:\n",
    "   - **Type**: `CategoricalParameter`\n",
    "   - **Values**: `['sqrt', 'log2']`\n",
    "   - **Description**: This parameter defines the number of features to consider when looking for the best split. \n",
    "     - `sqrt` uses the square root of the total number of features.\n",
    "     - `log2` uses the base-2 logarithm of the total number of features.\n",
    "   - Tuning this parameter helps control the randomness in the model, impacting both performance and generalization. The idea is to use a subset of features at each split, reducing the chance of overfitting by introducing some randomness.\n",
    "\n",
    "#### Sagemaker Tuning Configuration Parameters\n",
    "\n",
    "- **`max_jobs`**:\n",
    "  - **Description**: This defines the maximum number of hyperparameter tuning jobs to run. In the provided code, it is set to 100. This setting controls how many different hyperparameter combinations can be tested during the tuning process. A higher value increases the potential for finding the best hyperparameters but also consumes more resources.\n",
    "\n",
    "- **`max_parallel_jobs`**:\n",
    "  - **Description**: This specifies how many hyperparameter tuning jobs should run in parallel. In this case, the value is set to 2, meaning the system will execute up to two jobs simultaneously, allowing faster exploration of the hyperparameter space. Tuning this value balances between resource consumption and time efficiency.\n",
    "\n",
    "- **`strategy`**:\n",
    "  - **Description**: This defines the optimization strategy for hyperparameter tuning. The chosen strategy, \"Bayesian\", is a model-based optimization approach. It builds a probabilistic model of the objective function and uses this model to select the most promising hyperparameters to evaluate next. Bayesian optimization is efficient for high-dimensional search spaces and helps find the optimal parameters in fewer steps than random search.\n",
    "\n",
    "- **`objective_type`**:\n",
    "  - **Description**: This specifies the type of optimization. \"Minimize\" means the goal is to minimize the objective metric, in this case, the `validation:rmse` (Root Mean Square Error) metric. Minimizing this error improves the model's performance by reducing the difference between predicted and actual values.\n",
    "\n",
    "- **`early_stopping_type`**:\n",
    "  - **Description**: \"Auto\" means that early stopping will be automatically applied if the validation performance plateaus or doesn't improve. This prevents wasting resources on further jobs that aren't improving the model and ensures the tuning process is efficient by stopping early when no significant improvement is observed.\n",
    "\n",
    "These hyperparameter tuning parameters allow the model to optimize its performance by searching for the best combination of values for each hyperparameter, making it more accurate and efficient for the given task.\n",
    "\n",
    "\n",
    "#### Tuning Step:\n",
    "   - `step_tuning` creates a tuning step in a SageMaker pipeline:\n",
    "     - Uses the hyperparameter tuner (`tuner_log`).\n",
    "     - Specifies the training and validation data inputs (from previous pipeline steps).\n",
    "     - The data for training and validation is in CSV format and stored in S3 locations provided by the previous step (`step_process`).\n",
    "\n",
    "This code automates the training and hyperparameter tuning of a machine learning model using SageMaker's pipeline features, optimizing for validation RMSE (Root Mean Square Error).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d078bc1-4a9a-49cc-8270-e4065100bd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output path for the model artifacts from the Hyperparameter Tuning Job\n",
    "model_prefix = f\"{project_name}/AbsaZindiTrain\"\n",
    "model_path = f\"s3://{default_bucket}/{model_prefix}\"\n",
    "\n",
    "train_job_name =f\"{project_name}-training\"\n",
    "\n",
    "sklearn_estimator = SKLearn(\n",
    "    entry_point=\"code/train.py\",\n",
    "    output_dir=model_path,\n",
    "    output_path=model_path,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m4.xlarge\",\n",
    "    framework_version=framework_version,\n",
    "    base_job_name=train_job_name,\n",
    "    sagemaker_session=pipeline_session\n",
    ")\n",
    "\n",
    "objective_metric_name = \"validation:rmse\"\n",
    "metric_definitions = [{'Name': 'validation:rmse',\n",
    "                       'Regex': \".*validation:rmse ([0-9\\\\.]+).*\"},\n",
    "                      {'Name': 'train:rmse',\n",
    "                       'Regex': \".*train:rmse ([0-9\\\\.]+).*\"}\n",
    "                     ]\n",
    "\n",
    "hyperparameter_ranges = {\n",
    "    'n_estimators': IntegerParameter(50, 3500),  # Number of trees\n",
    "    'max_depth': IntegerParameter(3, 50),  # Tree depth\n",
    "    'min_samples_split': IntegerParameter(2, 50),  # Minimum samples to split a node\n",
    "    'min_samples_leaf': IntegerParameter(1, 40),  # Minimum samples in a leaf\n",
    "    'max_features': CategoricalParameter(['sqrt', 'log2']),  # Number of features per split\n",
    "}\n",
    "\n",
    "n_jobs = 100\n",
    "n_parallel_jobs = 2\n",
    "\n",
    "tuner_log = HyperparameterTuner(\n",
    "    estimator=sklearn_estimator,\n",
    "    objective_metric_name=objective_metric_name,\n",
    "    metric_definitions=metric_definitions,\n",
    "    hyperparameter_ranges=hyperparameter_ranges,\n",
    "    max_jobs=n_jobs,\n",
    "    max_parallel_jobs=n_parallel_jobs,\n",
    "    strategy=\"Bayesian\",\n",
    "    objective_type=\"Minimize\",\n",
    "    random_seed=200,\n",
    "    early_stopping_type=\"Auto\"\n",
    ")\n",
    "\n",
    "step_tuning = TuningStep(\n",
    "    name=\"AbsaZindiHPTuning\",\n",
    "    tuner=tuner_log,\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",\n",
    "        ),\n",
    "        \"validation\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"validation\"].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",\n",
    "        ),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5257cb86-95e7-4313-8209-ed7cb02a86d2",
   "metadata": {},
   "source": [
    "# Evaluation Step\n",
    "\n",
    "This script evaluates a trained model on a test dataset and generates a performance report, which is saved as a JSON file. Here's a detailed breakdown of what the script does:\n",
    "\n",
    "1. **Imports and Setup:**\n",
    "   - Various libraries are imported, including `json`, `pickle`, `tarfile`, `logging`, `joblib`, and `os`, for handling file operations, logging, model loading, and evaluation.\n",
    "   - It uses `numpy` and `pandas` for handling arrays and dataframes, and `mean_squared_error` from `sklearn.metrics` to calculate the performance metric (RMSE).\n",
    "\n",
    "2. **Logging Setup:**\n",
    "   - A logger is configured to capture log messages and display them on the console, with the logging level set to `INFO`.\n",
    "\n",
    "3. **Model Extraction:**\n",
    "   - The script assumes the model is saved as a `.tar.gz` archive in the `/opt/ml/processing/model/` directory. \n",
    "   - It extracts the model from the tarball using `tarfile.open` and extracts its contents.\n",
    "\n",
    "4. **Model Loading:**\n",
    "   - Once extracted, the model (assumed to be saved in `model.joblib`) is loaded using `joblib.load()`, which is commonly used for loading scikit-learn models.\n",
    "\n",
    "5. **Test Data Loading:**\n",
    "   - The script loads a CSV file containing the test data (`test.csv`) from the `/opt/ml/processing/test/` directory.\n",
    "   - The first column (`iloc[:, 0]`) is assumed to be the target variable (`y_test`), and the remaining columns (`iloc[:, 1:]`) are the features (`X_test`).\n",
    "\n",
    "6. **Prediction and Evaluation:**\n",
    "   - The model is used to predict the target variable based on the test features (`X_test`).\n",
    "   - The script calculates the **Root Mean Squared Error (RMSE)** between the true values (`y_test`) and the predicted values (`predictions`).\n",
    "   - It also calculates the **standard deviation** of the residuals (the difference between the actual and predicted values).\n",
    "\n",
    "7. **Results Reporting:**\n",
    "   - A dictionary (`report_dict`) is created to store the evaluation results, including the RMSE and standard deviation.\n",
    "   - This dictionary is printed to the console for reference.\n",
    "\n",
    "8. **Output Directory and Saving Report:**\n",
    "   - The script ensures the output directory (`/opt/ml/processing/evaluation`) exists, creating it if necessary using `pathlib.Path`.\n",
    "   - The evaluation report is then saved as a JSON file (`evaluation.json`) in the output directory.\n",
    "\n",
    "### File Operations:\n",
    "- **Extracting the model:** The model is expected to be in a tarball (`model.tar.gz`) and is extracted to the current working directory.\n",
    "- **Reading test data:** The test data is read from a CSV file.\n",
    "- **Saving the report:** The evaluation report is written to a JSON file (`evaluation.json`).\n",
    "\n",
    "This script is typically used in an automated machine learning pipeline, where it extracts, loads, and evaluates a trained model, then outputs the evaluation results for further analysis or reporting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e54ebe95-056e-4c71-8c70-52de6ce05e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/evaluation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/evaluation.py\n",
    "import json\n",
    "import pathlib\n",
    "import pickle\n",
    "import tarfile\n",
    "import logging\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = os.path.join(\"/opt/ml/processing/model\", \"model.tar.gz\")\n",
    "    output_dir = \"/opt/ml/processing/evaluation\"\n",
    "    \n",
    "    logger.info(\"Extracting model from path: {}\".format(model_path))\n",
    "    with tarfile.open(model_path) as tar:\n",
    "        tar.extractall(path=\".\")\n",
    "    print(\"Loading model\")\n",
    "    model = joblib.load(\"model.joblib\")\n",
    "\n",
    "    test_df = pd.read_csv(os.path.join(\"/opt/ml/processing/test\", \"test.csv\"), header=None)\n",
    "    y_test = test_df.iloc[:,0].values\n",
    "    X_test = test_df.iloc[:,1:].values\n",
    "    predictions = model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "    std = np.std(y_test - predictions)\n",
    "    \n",
    "    report_dict = {\n",
    "            \"regression_metrics\": {\n",
    "                \"rmse\": {\"value\": rmse, \"standard_deviation\": std},\n",
    "            }\n",
    "    }\n",
    "    \n",
    "    print(\"Classification report:\\n{}\".format(report_dict))\n",
    "    output_dir = \"/opt/ml/processing/evaluation\"\n",
    "    pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    evaluation_path = f\"{output_dir}/evaluation.json\"\n",
    "    with open(evaluation_path, \"w\") as f:\n",
    "        f.write(json.dumps(report_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a4dc38-11ef-4ed0-9de7-2d1d11fa5514",
   "metadata": {},
   "source": [
    "### Evaluation Step Setup\n",
    "\n",
    "This code is part of an Amazon SageMaker pipeline for evaluating and registering a model. It focuses on the evaluation of the top-performing model from a Hyperparameter Optimization (HPO) step and the subsequent registration of the model based on the evaluation results.\n",
    "\n",
    "Here's a detailed breakdown of the code:\n",
    "\n",
    "#### Evaluation Step Setup:\n",
    "   - **`script_eval`**: An instance of `SKLearnProcessor` is used for evaluating the model. This processor runs a script on a specified Sagemaker Managed scikit-learn container.\n",
    "     - **`framework_version`**: Specifies the version of the scikit-learn framework.\n",
    "     - **`instance_type`**: Specifies the instance type for the evaluation job (`ml.t3.large`).\n",
    "     - **`instance_count`**: Defines the number of instances for processing.\n",
    "     - **`base_job_name`**: Provides a base name for the evaluation job.\n",
    "     - **`role`**: The IAM role with appropriate permissions for SageMaker.\n",
    "     - **`sagemaker_session`**: The session that manages the communication with SageMaker.\n",
    "\n",
    "#### Evaluation Report Definition:\n",
    "   - **`evaluation_report`**: A `PropertyFile` that points to the evaluation report's output. This file contains the model's performance metrics in JSON format (`evaluation.json`). It will be used later in the pipeline to register model metrics.\n",
    "\n",
    "#### Evaluation Job Arguments (`eval_args`):\n",
    "   - **`script_eval.run()`**: Executes the evaluation step, providing the following inputs and outputs:\n",
    "     - **Inputs**:\n",
    "       - **Model Input**: The top-performing model from the HPO step is retrieved using `step_tuning.get_top_model_s3_uri()`. This model is stored in the `/opt/ml/processing/model` directory.\n",
    "       - **Test Data**: The test dataset is sourced from the output of a previous processing step (`step_process`), which is placed in the `/opt/ml/processing/test` directory.\n",
    "     - **Outputs**:\n",
    "       - **Evaluation Output**: The evaluation results (metrics) are written to `/opt/ml/processing/evaluation` and saved as `evaluation.json`.\n",
    "     - **`code`**: The path to the evaluation script (`code/evaluation.py`) that performs the evaluation.\n",
    "\n",
    "#### Evaluation Step (`step_eval`):\n",
    "   - **`ProcessingStep`**: Defines the evaluation step in the pipeline.\n",
    "     - **`name`**: The name of the step (\"AbsaZindiEvaluateTopModel\").\n",
    "     - **`step_args`**: The arguments passed to the `run()` function, including the inputs, outputs, and the evaluation script.\n",
    "     - **`property_files`**: The `evaluation_report` is added as a property file, making the evaluation report available to the pipeline.\n",
    "     - **`cache_config`**: Optionally, caching configuration to improve pipeline efficiency by reusing results from previous runs.\n",
    "\n",
    "#### Model Metrics Setup:\n",
    "   - **`model_metrics`**: Defines the model metrics to be registered in the Model Registry.\n",
    "     - **`model_statistics`**: Points to the `evaluation.json` file, which contains the performance metrics (e.g., RMSE) of the model. The `MetricsSource` specifies the S3 URI of the evaluation report.\n",
    "\n",
    "#### Process Flow:\n",
    "- The top-performing model from the HPO step is extracted and evaluated using the `evaluation.py` script.\n",
    "- The evaluation results are stored in `evaluation.json` and used for registering the model's performance metrics.\n",
    "- The model's metrics (from `evaluation.json`) are then registered in the Model Registry, making it available for future use (e.g., deployment).\n",
    "\n",
    "This approach can be extended to evaluate multiple models from the HPO step, providing flexibility in comparing and selecting the best-performing models based on their evaluation metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d5b3d42-15ac-4bba-aae9-d65854301799",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    }
   ],
   "source": [
    "# A ProcessingStep is used to evaluate the performance of a selected model from the HPO step. In this case, the top performing model\n",
    "# is evaluated. Based on the results of the evaluation, the model is registered into the Model Registry using a ConditionStep.\n",
    "\n",
    "script_eval = SKLearnProcessor(\n",
    "    framework_version=framework_version,\n",
    "    instance_type='ml.t3.large',\n",
    "    instance_count=processing_instance_count,\n",
    "    base_job_name=\"sklearn-absa-zindi-evaluation\",\n",
    "    role=role,\n",
    "    sagemaker_session=pipeline_session\n",
    ")\n",
    "\n",
    "evaluation_report = PropertyFile(\n",
    "    name=\"BestTuningModelEvaluationReport\",\n",
    "    output_name=\"evaluation\",\n",
    "    path=\"evaluation.json\",\n",
    ")\n",
    "\n",
    "eval_args = script_eval.run(\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=step_tuning.get_top_model_s3_uri(\n",
    "                top_k=0,\n",
    "                s3_bucket=default_bucket,\n",
    "                prefix=model_prefix\n",
    "            ),\n",
    "            destination=\"/opt/ml/processing/model\",\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=step_process.properties.ProcessingOutputConfig.Outputs[\"test\"].S3Output.S3Uri,\n",
    "            destination=\"/opt/ml/processing/test\",\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\"),\n",
    "    ],\n",
    "    code=\"code/evaluation.py\",\n",
    ")\n",
    "\n",
    "# This can be extended to evaluate multiple models from the HPO step\n",
    "step_eval = ProcessingStep(\n",
    "    name=\"AbsaZindiEvaluateTopModel\",\n",
    "    step_args=eval_args,\n",
    "    property_files=[evaluation_report],\n",
    "    cache_config=cache_config,\n",
    ")\n",
    "\n",
    "model_metrics = ModelMetrics(\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri=\"{}/evaluation.json\".format(\n",
    "            step_eval.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n",
    "        ),\n",
    "        content_type=\"application/json\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27cd413-23a1-421e-b6e5-89fa90820551",
   "metadata": {},
   "source": [
    "# Model Deployment\n",
    "\n",
    "\n",
    "### Code Explanation: `inference.py`\n",
    "\n",
    "This script defines the necessary functions for handling model inference in a SageMaker deployment. These functions are designed to load a model, parse input data, make predictions, and format the response based on the content type. Here’s an explanation of each part of the script:\n",
    "\n",
    "#### `model_fn(model_dir)`\n",
    "   - **Purpose**: Loads the trained model.\n",
    "   - **Input**: `model_dir` – The directory where the model is stored.\n",
    "   - **Output**: Returns the loaded model.\n",
    "   - **Details**: The model is loaded using `joblib.load()` from the directory specified by `model_dir`. This function is invoked during the inference process when the model is first loaded for use.\n",
    "\n",
    "#### `input_fn(request_body, request_content_type)`\n",
    "   - **Purpose**: Parses the input data from the request body based on the content type.\n",
    "   - **Input**:\n",
    "     - `request_body`: The raw body of the request (either JSON or CSV).\n",
    "     - `request_content_type`: The type of the content being passed (either `\"application/json\"` or `\"text/csv\"`).\n",
    "   - **Output**: Returns a Pandas DataFrame containing the input data.\n",
    "   - **Details**:\n",
    "     - If the content type is `\"application/json\"`, the function parses the JSON data and converts it into a Pandas DataFrame.\n",
    "     - If the content type is `\"text/csv\"`, the function parses the CSV data, splits it into rows, and constructs a DataFrame. It assumes the first column is the customer identifier (`CUSTOMER_IDENTIFIER`), and it uses all other columns as features, excluding the first two (this can be adjusted based on the actual structure of the data).\n",
    "     - If an unsupported content type is provided, it raises a `ValueError`.\n",
    "\n",
    "#### `predict_fn(input_data, model)`\n",
    "   - **Purpose**: Makes predictions using the input data and the trained model.\n",
    "   - **Input**:\n",
    "     - `input_data`: The preprocessed input data as a Pandas DataFrame.\n",
    "     - `model`: The loaded model used to make predictions.\n",
    "   - **Output**: A dictionary mapping customer identifiers to their respective predictions.\n",
    "   - **Details**: The function converts the input data (`input_data`) into a NumPy array (`X_input`) and uses the model to predict the outcomes. The `cust_ids` (customer identifiers) are extracted from the DataFrame’s index, and the predictions are returned in a dictionary, where the keys are customer IDs and the values are the predictions.\n",
    "\n",
    "#### `output_fn(prediction, response_content_type)`\n",
    "   - **Purpose**: Formats the predictions for the response, based on the requested content type.\n",
    "   - **Input**:\n",
    "     - `prediction`: The prediction results in the form of a dictionary (customer identifier to prediction).\n",
    "     - `response_content_type`: The desired response format, either `\"application/json\"` or `\"text/csv\"`.\n",
    "   - **Output**: The predictions are formatted as a string (in JSON or CSV format) to be returned as a response.\n",
    "   - **Details**:\n",
    "     - If the response content type is `\"application/json\"`, the predictions are converted to a CSV string (the commented-out line shows how to convert to JSON, if desired).\n",
    "     - If the response content type is `\"text/csv\"`, the predictions are converted to CSV format.\n",
    "     - If an unsupported content type is provided, it raises a `ValueError`.\n",
    "\n",
    "#### Process Flow:\n",
    "1. The **`model_fn`** function is invoked to load the model from the directory during the initialization of the endpoint.\n",
    "2. The **`input_fn`** function is invoked to parse the incoming request based on the content type (either JSON or CSV).\n",
    "3. The **`predict_fn`** function makes predictions using the input data and the loaded model.\n",
    "4. The **`output_fn`** function formats the predictions in the requested content type (either JSON or CSV) for the response.\n",
    "\n",
    "This script provides a robust way to handle inference requests with different data formats and ensures that the response is properly formatted for the client.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "393bab10-112d-48ee-9dc2-924b0a6dfb02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/inference.py\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    model = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n",
    "    return model\n",
    "\n",
    "def input_fn(request_body, request_content_type):\n",
    "    \"\"\"\n",
    "    Parse the request body.\n",
    "    \"\"\"\n",
    "    if request_content_type == \"application/json\":\n",
    "        payload = json.loads(request_body)\n",
    "        return pd.DataFrame([payload])\n",
    "    elif request_content_type == \"text/csv\":\n",
    "        # Convert CSV row into a Pandas DataFrame\n",
    "        data = [row.split(\",\") for row in request_body.strip().split(\"\\n\")]\n",
    "        df = pd.DataFrame(data[1:], columns=data[0])\n",
    "        df.set_index(\"CUSTOMER_IDENTIFIER\", inplace=True)\n",
    "        return df.iloc[:, 2:]  # Adjust this based on actual data structure\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported content type: {request_content_type}\")\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    # Convert to NumPy array for prediction\n",
    "    X_input = input_data.values\n",
    "    # Predict\n",
    "    predictions = model.predict(X_input)\n",
    "    cust_ids = input_data.index\n",
    "    return dict(zip(cust_ids, predictions))\n",
    "\n",
    "def output_fn(prediction, response_content_type):\n",
    "    \"\"\"\n",
    "    Format the prediction for response.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(list(prediction.items()), \n",
    "                      columns=[\"CUSTOMER_IDENTIFIER\", \"PREDICTION\"])\n",
    "    if response_content_type == \"application/json\":\n",
    "        return df.to_csv(index=False)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported response content type: {response_content_type}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4697bee9-df70-4b05-92ef-01e9b00591a6",
   "metadata": {},
   "source": [
    "### Code Explanation: Model Registration and Creation in SageMaker Pipeline\n",
    "\n",
    "This script handles the creation and registration of the best model obtained from the Hyperparameter Optimization (HPO) step in an Amazon SageMaker pipeline. Here's a breakdown of the components:\n",
    "\n",
    "#### Creating the Best Model (`best_model`)**:\n",
    "   - **`SKLearnModel`**: Defines the best model using the scikit-learn framework.\n",
    "     - **`framework_version`**: The version of the scikit-learn framework used to train the model.\n",
    "     - **`model_data`**: The URI of the top-performing model from the HPO step. The model is retrieved using `step_tuning.get_top_model_s3_uri()`. This URI points to the model artifact stored in Amazon S3.\n",
    "     - **`entry_point`**: The script that will be used for inference (`code/inference.py`).\n",
    "     - **`sagemaker_session`**: The session that facilitates interaction with Amazon SageMaker.\n",
    "     - **`role`**: The IAM role with permissions to access the necessary resources (such as S3 and SageMaker).\n",
    "\n",
    "#### Model Creation Step (`step_create_best_model`)**:\n",
    "   - **`ModelStep`**: A SageMaker Pipeline step that creates the model from the best model package.\n",
    "     - **`name`**: The name of the step (\"AbsaZindiCreateBestModel\").\n",
    "     - **`step_args`**: The arguments passed to the `create()` method. The model is created using an instance type of `ml.m5.xlarge` for deployment.\n",
    "\n",
    "#### Model Registration Arguments (`register_args`)**:\n",
    "   - **`best_model.register()`**: Registers the best model in the SageMaker Model Registry with the following configurations:\n",
    "     - **`content_types`**: Specifies that the model accepts `\"text/csv\"` as input data format.\n",
    "     - **`response_types`**: Specifies that the model outputs predictions in `\"text/csv\"` format.\n",
    "     - **`inference_instances`**: Specifies the instance types on which the model can be deployed for inference, in this case, `\"ml.t2.medium\"` and `\"ml.m5.large\"`.\n",
    "     - **`transform_instances`**: Specifies the instance types for batch transformations, in this case, `\"ml.m5.large\"`.\n",
    "     - **`model_package_group_name`**: The name of the model package group in the Model Registry where the model will be stored. This helps organize models by version or type.\n",
    "     - **`approval_status`**: The approval status for the model. This can be either `Approved` or `PendingManualApproval`, determining if the model is immediately deployable or needs manual approval first.\n",
    "\n",
    "#### Model Registration Step (`step_register_best`)**:\n",
    "   - **`ModelStep`**: A SageMaker Pipeline step that registers the model in the Model Registry.\n",
    "     - **`name`**: The name of the step (\"AbsaZindiRegisterBestModel\").\n",
    "     - **`step_args`**: The arguments passed to the `register()` method to register the model.\n",
    "\n",
    "#### Process Flow:\n",
    "1. **Model Creation**: The top-performing model from the HPO step is selected and used to create an `SKLearnModel` object.\n",
    "2. **Model Deployment**: The model is created using the specified instance type (`ml.m5.xlarge`).\n",
    "3. **Model Registration**: The model is registered in the Model Registry under the specified model package group and with the given approval status. This makes the model version available for deployment or further evaluation.\n",
    "4. **Model Versioning**: Multiple models can be added to the same model package group with different versions, ensuring that model versions are managed and stored properly.\n",
    "\n",
    "#### Considerations:\n",
    "- You can add the same model package group as different versions (e.g., for models that evolve over time). The `model_package_group_name` ensures that all versions of the model are grouped together.\n",
    "- If the approval status is set to `PendingManualApproval`, the model will not be deployed until manually approved, providing a gate for model deployment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df0bb399-afb4-4f08-a3f1-9b0c93e8b7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same model package group as different versions within the group or the models can be added to different model package groups.\n",
    "best_model = SKLearnModel(\n",
    "    framework_version=framework_version,\n",
    "    model_data=step_tuning.get_top_model_s3_uri(\n",
    "        top_k=0,\n",
    "        s3_bucket=default_bucket, \n",
    "        prefix=model_prefix \n",
    "    ),\n",
    "    entry_point='code/inference.py',\n",
    "    sagemaker_session=pipeline_session,\n",
    "    role=role\n",
    ")\n",
    "\n",
    "step_create_best_model = ModelStep(\n",
    "    name=\"AbsaZindiCreateBestModel\",\n",
    "    step_args=best_model.create(instance_type=\"ml.m5.xlarge\"),\n",
    ")\n",
    "\n",
    "register_args = best_model.register(\n",
    "    content_types=[\"text/csv\"],\n",
    "    response_types=[\"text/csv\"],\n",
    "    inference_instances=[\"ml.t2.medium\", \"ml.m5.large\"],\n",
    "    transform_instances=[\"ml.m5.large\"],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    approval_status=model_approval_status,\n",
    ")\n",
    "\n",
    "step_register_best = ModelStep(name=\"AbsaZindiRegisterBestModel\", step_args=register_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734857d6-d854-4ef0-8175-3fd6bb78a7af",
   "metadata": {},
   "source": [
    "# Batch Tranform\n",
    "\n",
    "### Amazon SageMaker Batch Transform\n",
    "\n",
    "**Amazon SageMaker Batch Transform** is a managed service that allows you to run inferences on large datasets in batch mode. Unlike real-time inference (where you get immediate predictions via an endpoint), batch transform allows you to process datasets in bulk and get predictions without the need for real-time access.\n",
    "\n",
    "Here’s how it works and its key components:\n",
    "\n",
    "#### Purpose\n",
    "   - **Batch Processing**: Batch Transform is designed to process large datasets, typically when real-time inference is not necessary.\n",
    "   - **Offline Prediction**: You provide a dataset to SageMaker, and it runs inference on that dataset in the background.\n",
    "   - **Scalability**: SageMaker Batch Transform can scale according to the size of the data and the computational requirements. It can be processed in parallel on multiple instances.\n",
    "\n",
    "#### How it Works\n",
    "   - **Step 1: Model Preparation**\n",
    "     - First, you train a machine learning model using Amazon SageMaker or bring your own pre-trained model.\n",
    "   - **Step 2: Transform Job Setup**\n",
    "     - You define a **transform job** where you specify the model to use, the input data (which is typically stored in S3), and the compute resources (instance type and count).\n",
    "   - **Step 3: Inference Execution**\n",
    "     - Once the transform job is started, SageMaker automatically launches the required compute resources (instances), loads the model, and runs inference on the input dataset.\n",
    "   - **Step 4: Output Storage**\n",
    "     - The results of the inference are written to an S3 bucket in the output path specified.\n",
    "  \n",
    "#### Components\n",
    "   - **Model**: The pre-trained model that will be used to make predictions.\n",
    "   - **Input Data**: The data on which the model will perform inference. It should be stored in an S3 bucket.\n",
    "   - **Compute Resources**: The type of instance (e.g., `ml.m4.xlarge`) and the number of instances that will be used for processing.\n",
    "   - **Output Data**: The location in an S3 bucket where the results of the inference will be saved.\n",
    "   - **TransformStep**: The step in your pipeline that runs the batch transform job.\n",
    "\n",
    "#### Key Features\n",
    "   - **No Need for Real-Time Endpoints**: Unlike deploying models via SageMaker Endpoints for real-time inference, Batch Transform processes data asynchronously.\n",
    "   - **Scalable**: You can scale up or down the number of instances depending on the volume of data and processing time requirements.\n",
    "   - **Supports Various Data Formats**: Batch Transform supports input and output data in various formats like CSV, JSON, Parquet, etc.\n",
    "   - **Flexible Compute Options**: You can specify instance types and count based on the resources needed for your job.\n",
    "   - **Cost-Effective**: Since you only pay for the compute resources used during the batch transform process, it can be more cost-efficient for large datasets compared to using real-time inference endpoints.\n",
    "   - **Asynchronous**: The job is processed in the background, and you can retrieve the results once the job completes.\n",
    "\n",
    "#### Steps in a Batch Transform Process\n",
    "   1. **Prepare Data**: Store your input data in an S3 bucket.\n",
    "   2. **Create Model**: Either use an existing trained model or train a new model with SageMaker.\n",
    "   3. **Setup Transform Job**: Specify the input, output, model, and instance configuration (instance type, instance count).\n",
    "   4. **Run Job**: Launch the batch transform job, which will process the data in parallel across instances.\n",
    "   5. **Retrieve Results**: Once the job completes, the output is available in the specified S3 output location.\n",
    "\n",
    "#### Typical Use Cases\n",
    "   - **Large-Scale Predictions**: When you have large datasets that need predictions but don't require real-time access to the model.\n",
    "   - **Data Preprocessing**: When you need to apply a model to datasets stored in large files, such as log data or sensor data, and don’t require immediate responses.\n",
    "   - **Cost-Effective Batch Processing**: When you need to process data in batch but don’t want to maintain a real-time inference endpoint.\n",
    "----\n",
    "\n",
    "#### Transformer Initialization\n",
    "   - `transformer = Transformer(...)`: This initializes a transformer object that is used to run inference (transform) on a dataset using our best model.\n",
    "     - `model_name=step_create_best_model.properties.ModelName`: Specifies the model to use, which is retrieved from the properties of the previous step (`step_create_best_model`).\n",
    "     - `instance_type='ml.m4.xlarge'`: Specifies the type of machine instance to use for the transform job. In this case, it uses `ml.m4.xlarge`, an AWS machine learning instance with balanced compute and memory.\n",
    "     - `instance_count=1`: Sets the number of instances to use for the job. Here, it's set to one.\n",
    "     - `output_path=f\"s3://{default_bucket}/AbsaZindiTransform\"`: Defines where the output of the transform job will be stored, using an S3 path. The path includes the `default_bucket` variable, which holds the S3 bucket name.\n",
    "\n",
    "#### Transform Step Initialization\n",
    "   - `step_transform = TransformStep(...)`: This creates a transform step within a workflow using the initialized transformer.\n",
    "     - `name=\"AbsaZindiTransform\"`: Specifies the name of the transform step.\n",
    "     - `transformer=transformer`: Assigns the previously initialized transformer to this step.\n",
    "     - `inputs=TransformInput(...)`: Defines the input configuration for the transform job.\n",
    "       - `data=step_process.properties.ProcessingOutputConfig.Outputs[\"to_be_submitted\"].S3Output.S3Uri`: Specifies the location of the input data, which is the output from the previous processing step (`step_process`), stored in S3.\n",
    "       - `content_type=\"text/csv\"`: Indicates the format of the input data. In this case, the data is in CSV format.\n",
    "       - `split_type='Line'`: Specifies how the data should be split for the transform job. In this case, it's split by lines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97353e08-fcf7-4a26-b388-c7fa054c495d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    model_name=step_create_best_model.properties.ModelName,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    instance_count=1,\n",
    "    output_path=f\"s3://{default_bucket}/AbsaZindiTransform\"\n",
    ")\n",
    "\n",
    "step_transform = TransformStep(\n",
    "    name=\"AbsaZindiTransform\", \n",
    "    transformer=transformer, \n",
    "    inputs=TransformInput(\n",
    "        data=step_process.properties.ProcessingOutputConfig.Outputs[\"to_be_submitted\"].S3Output.S3Uri,\n",
    "        content_type=\"text/csv\", \n",
    "        split_type='Line')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0ddf36-1606-47ae-996b-0dad5a759e80",
   "metadata": {},
   "source": [
    "### Code Explanation: Condition Step for Model Evaluation and Branching\n",
    "\n",
    "This code introduces a conditional step in the SageMaker Pipeline, where the execution is branched based on the evaluation of the model's performance. Specifically, it checks whether the model's RMSE (Root Mean Squared Error) is below a certain threshold (`rmse_score_threshold`). Here's a breakdown of the components:\n",
    "\n",
    "#### Condition (`cond_lte`):\n",
    "   - **`ConditionLessThanOrEqualTo`**: This condition checks if the value of the RMSE metric from the evaluation step is less than or equal to a predefined threshold.\n",
    "     - **`left`**: This specifies the value to compare, which is the RMSE value extracted from the evaluation report. The `JsonGet` function is used to extract the value from the JSON output of the evaluation step.\n",
    "       - **`step_name`**: The name of the evaluation step (`step_eval`).\n",
    "       - **`property_file`**: The property file (`evaluation_report`) that contains the evaluation results.\n",
    "       - **`json_path`**: The JSON path `\"regression_metrics.rmse.value\"` is used to extract the RMSE value from the report.\n",
    "     - **`right`**: This is the threshold value (`rmse_score_threshold`) that the RMSE is compared against.\n",
    "\n",
    "#### ConditionStep Object (`step_cond`):\n",
    "   - **`ConditionStep`**: This step performs conditional branching based on the result of the condition (`cond_lte`).\n",
    "     - **`name`**: The name of the step (\"AbsaZindiCheckRMSE\").\n",
    "     - **`conditions`**: The list of conditions that need to be evaluated. In this case, it is the `cond_lte` condition.\n",
    "     - **`if_steps`**: The steps to execute if the condition evaluates to `True` (i.e., RMSE is less than or equal to the threshold). If the condition is met, the following steps are executed:\n",
    "       - **`step_register_best`**: Registers the best model in the model registry.\n",
    "       - **`step_create_best_model`**: Creates the best model from the HPO results.\n",
    "       - **`step_transform`**: Runs the transformation job to apply the model to the input data.\n",
    "     - **`else_steps`**: The steps to execute if the condition evaluates to `False` (i.e., RMSE is greater than the threshold). In this case, the pipeline will fail the execution:\n",
    "       - **`step_fail`**: A failure step, which indicates that the model does not meet the required performance criteria.\n",
    "\n",
    "#### Process Flow:\n",
    "1. **Condition Evaluation**: The pipeline evaluates the condition `cond_lte`, comparing the model's RMSE value from the evaluation step with the predefined threshold (`rmse_score_threshold`).\n",
    "2. **Branching Execution**: \n",
    "   - If the model's RMSE is less than or equal to the threshold, the pipeline proceeds to register the model, create the model, and perform the transformation using the steps defined in `if_steps`.\n",
    "   - If the model's RMSE is greater than the threshold, the pipeline branches to the `else_steps` and fails the execution by running `step_fail`.\n",
    "\n",
    "#### Key Concepts:\n",
    "- **Conditional Execution**: This allows the pipeline to dynamically decide which steps to execute based on the outcome of a previous step. In this case, the decision is based on the model's evaluation performance (RMSE).\n",
    "- **Model Quality Gate**: The condition acts as a gate to ensure that only models that meet the required performance criteria (RMSE below the threshold) proceed to further steps like registration and transformation.\n",
    "- **Pipeline Branching**: The `if_steps` and `else_steps` allow different execution paths, which can be useful for handling different outcomes (e.g., model performance issues or failure).\n",
    "\n",
    "This step ensures that the model only moves forward if it meets the required performance threshold, enabling controlled deployment and further processing in the pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "437cb549-472e-445b-8389-34a8b96541ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_fail = FailStep(\n",
    "    name=\"AbsaZindiRMSEScoreFail\",\n",
    "    error_message=Join(on=\" \", values=[\"Execution failed due to root mean squared error >\",rmse_score_threshold]),\n",
    ")\n",
    "\n",
    "\n",
    "# condition step for evaluating model quality and branching execution\n",
    "cond_lte = ConditionLessThanOrEqualTo(\n",
    "    left=JsonGet(\n",
    "        step_name=step_eval.name,\n",
    "        property_file=evaluation_report,\n",
    "        json_path=\"regression_metrics.rmse.value\",\n",
    "    ),\n",
    "    right=rmse_score_threshold,\n",
    ")\n",
    "step_cond = ConditionStep(\n",
    "    name=\"AbsaZindiCheckRMSE\",\n",
    "    conditions=[cond_lte],\n",
    "    if_steps=[step_register_best, step_create_best_model, step_transform],\n",
    "    else_steps=[step_fail],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c05c1d-afb5-4b68-ab0b-3ac218ab4321",
   "metadata": {},
   "source": [
    "### Code Explanation: SageMaker Pipeline Creation, Execution, and Monitoring\n",
    "\n",
    "This code sets up and executes a SageMaker pipeline that includes data processing, hyperparameter tuning, model evaluation, and conditional branching. Here's a breakdown of the components:\n",
    "\n",
    "#### Pipeline Object (`pipeline`):\n",
    "   - **`Pipeline`**: This is the main class used to define the workflow and the steps in SageMaker.\n",
    "     - **`name`**: The name of the pipeline, which in this case is set to `\"AbsaZindiPipeline\"`.\n",
    "     - **`parameters`**: A list of parameters required for the pipeline execution. These include:\n",
    "       - **`processing_instance_count`**: The number of instances to be used for processing.\n",
    "       - **`instance_type`**: The instance type to be used for model training, processing, and other steps.\n",
    "       - **`model_approval_status`**: Defines the approval status for the model (e.g., \"Approved\" or \"Pending\").\n",
    "       - **`train_data`, `test_data`, `customer_data`, `transactions_data`**: These parameters hold the S3 paths to the data required for training, testing, and other steps.\n",
    "       - **`rmse_score_threshold`**: The RMSE threshold used for conditional branching to evaluate whether the model meets the required performance.\n",
    "     - **`steps`**: A list of steps in the pipeline. These are the individual actions that make up the workflow, which include:\n",
    "       - **`step_process`**: The data preprocessing step.\n",
    "       - **`step_tuning`**: The hyperparameter tuning step.\n",
    "       - **`step_eval`**: The model evaluation step.\n",
    "       - **`step_cond`**: The condition step that branches execution based on the evaluation results (RMSE).\n",
    "\n",
    "#### Pipeline Definition (`definition`):\n",
    "   - **`pipeline.definition()`**: This method returns the pipeline definition in JSON format, which is useful for inspecting or debugging the pipeline structure.\n",
    "   - **`json.loads(pipeline.definition())`**: This parses the JSON definition of the pipeline and prints it to the console.\n",
    "\n",
    "3. **Pipeline Upsert (`pipeline.upsert()`)**:\n",
    "   - **`upsert`**: This method uploads (or updates) the pipeline definition to Amazon SageMaker. The `role_arn` is passed to grant the necessary permissions for the pipeline execution.\n",
    "     - **`role_arn`**: The Amazon Resource Name (ARN) of the role that SageMaker uses for the pipeline. This role must have sufficient permissions to access S3 buckets, run training jobs, etc.\n",
    "\n",
    "4. **Pipeline Execution (`pipeline.start()`)**:\n",
    "   - **`start()`**: This method starts the execution of the pipeline. Once the pipeline is upserted, this command triggers the actual execution of the defined steps.\n",
    "\n",
    "#### Process Flow:\n",
    "1. **Pipeline Definition**: The pipeline is defined with specific parameters, steps, and conditions, which form the entire workflow.\n",
    "2. **Upserting the Pipeline**: The pipeline definition is uploaded to SageMaker, allowing it to be executed in the SageMaker environment.\n",
    "3. **Starting the Execution**: After the pipeline is upserted, the execution is triggered, and SageMaker begins executing the steps in the pipeline (from data processing to model evaluation and conditional branching).\n",
    "\n",
    "#### Key Concepts:\n",
    "- **Pipeline as Code**: SageMaker allows you to define and manage ML workflows as code. This enables reproducibility, versioning, and automation of the ML lifecycle.\n",
    "- **Parameters**: Parameters are used to make the pipeline flexible and configurable. They allow you to pass data or configurations to the pipeline when executing it.\n",
    "- **Pipeline Steps**: Each step represents an action within the pipeline, such as data processing, training, evaluation, and conditional branching.\n",
    "- **Upsert and Start Execution**: The `upsert` method ensures that the pipeline definition is current in SageMaker, while `start()` triggers the actual running of the pipeline, making the entire process automated.\n",
    "\n",
    "This code defines, uploads, and executes a full machine learning pipeline that automates key steps such as data processing, training, hyperparameter tuning, evaluation, and model registration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1cdc57c9-95f9-4c29-9905-f7503250821e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Metadata': {},\n",
      " 'Parameters': [{'DefaultValue': 1,\n",
      "                 'Name': 'ProcessingInstanceCount',\n",
      "                 'Type': 'Integer'},\n",
      "                {'DefaultValue': 'ml.m4.xlarge',\n",
      "                 'Name': 'TrainingInstanceType',\n",
      "                 'Type': 'String'},\n",
      "                {'DefaultValue': 'PendingManualApproval',\n",
      "                 'Name': 'ModelApprovalStatus',\n",
      "                 'Type': 'String'},\n",
      "                {'DefaultValue': 's3://sagemaker-us-east-1-770208914484/absa_zindi_challenge/input-data/train/Train.csv',\n",
      "                 'Name': 'TrainData',\n",
      "                 'Type': 'String'},\n",
      "                {'DefaultValue': 's3://sagemaker-us-east-1-770208914484/absa_zindi_challenge/input-data/test/Test.csv',\n",
      "                 'Name': 'TestData',\n",
      "                 'Type': 'String'},\n",
      "                {'DefaultValue': 's3://sagemaker-us-east-1-770208914484/absa_zindi_challenge/input-data/customer/customer.csv',\n",
      "                 'Name': 'CustomerData',\n",
      "                 'Type': 'String'},\n",
      "                {'DefaultValue': 's3://sagemaker-us-east-1-770208914484/absa_zindi_challenge/input-data/transactions/transactions.csv',\n",
      "                 'Name': 'TransactionsData',\n",
      "                 'Type': 'String'},\n",
      "                {'DefaultValue': 6500.0,\n",
      "                 'Name': 'RMSEThreshold',\n",
      "                 'Type': 'Float'}],\n",
      " 'PipelineExperimentConfig': {'ExperimentName': {'Get': 'Execution.PipelineName'},\n",
      "                              'TrialName': {'Get': 'Execution.PipelineExecutionId'}},\n",
      " 'Steps': [{'Arguments': {'AppSpecification': {'ContainerArguments': ['--train-size',\n",
      "                                                                      '0.9',\n",
      "                                                                      '--random_state',\n",
      "                                                                      '100'],\n",
      "                                               'ContainerEntrypoint': ['python3',\n",
      "                                                                       '/opt/ml/processing/input/code/preprocessing.py'],\n",
      "                                               'ImageUri': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:1.2-1-cpu-py3'},\n",
      "                          'ProcessingInputs': [{'AppManaged': False,\n",
      "                                                'InputName': 'input-1',\n",
      "                                                'S3Input': {'LocalPath': '/opt/ml/processing/input/train',\n",
      "                                                            'S3CompressionType': 'None',\n",
      "                                                            'S3DataDistributionType': 'FullyReplicated',\n",
      "                                                            'S3DataType': 'S3Prefix',\n",
      "                                                            'S3InputMode': 'File',\n",
      "                                                            'S3Uri': 's3://sagemaker-us-east-1-770208914484/absa_zindi_challenge/input-data/train/Train.csv'}},\n",
      "                                               {'AppManaged': False,\n",
      "                                                'InputName': 'input-2',\n",
      "                                                'S3Input': {'LocalPath': '/opt/ml/processing/input/test',\n",
      "                                                            'S3CompressionType': 'None',\n",
      "                                                            'S3DataDistributionType': 'FullyReplicated',\n",
      "                                                            'S3DataType': 'S3Prefix',\n",
      "                                                            'S3InputMode': 'File',\n",
      "                                                            'S3Uri': 's3://sagemaker-us-east-1-770208914484/absa_zindi_challenge/input-data/test/Test.csv'}},\n",
      "                                               {'AppManaged': False,\n",
      "                                                'InputName': 'input-3',\n",
      "                                                'S3Input': {'LocalPath': '/opt/ml/processing/input/transactions',\n",
      "                                                            'S3CompressionType': 'None',\n",
      "                                                            'S3DataDistributionType': 'FullyReplicated',\n",
      "                                                            'S3DataType': 'S3Prefix',\n",
      "                                                            'S3InputMode': 'File',\n",
      "                                                            'S3Uri': 's3://sagemaker-us-east-1-770208914484/absa_zindi_challenge/input-data/transactions/transactions.csv'}},\n",
      "                                               {'AppManaged': False,\n",
      "                                                'InputName': 'input-4',\n",
      "                                                'S3Input': {'LocalPath': '/opt/ml/processing/input/customer',\n",
      "                                                            'S3CompressionType': 'None',\n",
      "                                                            'S3DataDistributionType': 'FullyReplicated',\n",
      "                                                            'S3DataType': 'S3Prefix',\n",
      "                                                            'S3InputMode': 'File',\n",
      "                                                            'S3Uri': 's3://sagemaker-us-east-1-770208914484/absa_zindi_challenge/input-data/customer/customer.csv'}},\n",
      "                                               {'AppManaged': False,\n",
      "                                                'InputName': 'code',\n",
      "                                                'S3Input': {'LocalPath': '/opt/ml/processing/input/code',\n",
      "                                                            'S3CompressionType': 'None',\n",
      "                                                            'S3DataDistributionType': 'FullyReplicated',\n",
      "                                                            'S3DataType': 'S3Prefix',\n",
      "                                                            'S3InputMode': 'File',\n",
      "                                                            'S3Uri': 's3://sagemaker-us-east-1-770208914484/AbsaZindiPipeline/code/7ed234d34a078ad6cf0714fc4fe9be41/preprocessing.py'}}],\n",
      "                          'ProcessingOutputConfig': {'Outputs': [{'AppManaged': False,\n",
      "                                                                  'OutputName': 'train',\n",
      "                                                                  'S3Output': {'LocalPath': '/opt/ml/processing/output/train',\n",
      "                                                                               'S3UploadMode': 'EndOfJob',\n",
      "                                                                               'S3Uri': 's3://sagemaker-us-east-1-770208914484/absa_zindi_challenge/opt/ml/processing/output/train'}},\n",
      "                                                                 {'AppManaged': False,\n",
      "                                                                  'OutputName': 'validation',\n",
      "                                                                  'S3Output': {'LocalPath': '/opt/ml/processing/output/validation',\n",
      "                                                                               'S3UploadMode': 'EndOfJob',\n",
      "                                                                               'S3Uri': 's3://sagemaker-us-east-1-770208914484/absa_zindi_challenge/opt/ml/processing/output/validation'}},\n",
      "                                                                 {'AppManaged': False,\n",
      "                                                                  'OutputName': 'test',\n",
      "                                                                  'S3Output': {'LocalPath': '/opt/ml/processing/output/test',\n",
      "                                                                               'S3UploadMode': 'EndOfJob',\n",
      "                                                                               'S3Uri': 's3://sagemaker-us-east-1-770208914484/absa_zindi_challenge/opt/ml/processing/output/test'}},\n",
      "                                                                 {'AppManaged': False,\n",
      "                                                                  'OutputName': 'to_be_submitted',\n",
      "                                                                  'S3Output': {'LocalPath': '/opt/ml/processing/output/to_be_submitted',\n",
      "                                                                               'S3UploadMode': 'EndOfJob',\n",
      "                                                                               'S3Uri': 's3://sagemaker-us-east-1-770208914484/absa_zindi_challenge/opt/ml/processing/output/to_be_submitted'}},\n",
      "                                                                 {'AppManaged': False,\n",
      "                                                                  'OutputName': 'artifacts',\n",
      "                                                                  'S3Output': {'LocalPath': '/opt/ml/processing/output/artifacts',\n",
      "                                                                               'S3UploadMode': 'EndOfJob',\n",
      "                                                                               'S3Uri': 's3://sagemaker-us-east-1-770208914484/absa_zindi_challenge/opt/ml/processing/output/artifacts'}}]},\n",
      "                          'ProcessingResources': {'ClusterConfig': {'InstanceCount': {'Get': 'Parameters.ProcessingInstanceCount'},\n",
      "                                                                    'InstanceType': 'ml.t3.large',\n",
      "                                                                    'VolumeSizeInGB': 30}},\n",
      "                          'RoleArn': 'arn:aws:iam::770208914484:role/service-role/AmazonSageMaker-ExecutionRole-20241229T183387'},\n",
      "            'Name': 'AbsaZindiProcess',\n",
      "            'Type': 'Processing'},\n",
      "           {'Arguments': {'HyperParameterTuningJobConfig': {'HyperParameterTuningJobObjective': {'MetricName': 'validation:rmse',\n",
      "                                                                                                 'Type': 'Minimize'},\n",
      "                                                            'ParameterRanges': {'CategoricalParameterRanges': [{'Name': 'max_features',\n",
      "                                                                                                                'Values': ['\"sqrt\"',\n",
      "                                                                                                                           '\"log2\"']}],\n",
      "                                                                                'ContinuousParameterRanges': [],\n",
      "                                                                                'IntegerParameterRanges': [{'MaxValue': '3500',\n",
      "                                                                                                            'MinValue': '50',\n",
      "                                                                                                            'Name': 'n_estimators',\n",
      "                                                                                                            'ScalingType': 'Auto'},\n",
      "                                                                                                           {'MaxValue': '50',\n",
      "                                                                                                            'MinValue': '3',\n",
      "                                                                                                            'Name': 'max_depth',\n",
      "                                                                                                            'ScalingType': 'Auto'},\n",
      "                                                                                                           {'MaxValue': '50',\n",
      "                                                                                                            'MinValue': '2',\n",
      "                                                                                                            'Name': 'min_samples_split',\n",
      "                                                                                                            'ScalingType': 'Auto'},\n",
      "                                                                                                           {'MaxValue': '40',\n",
      "                                                                                                            'MinValue': '1',\n",
      "                                                                                                            'Name': 'min_samples_leaf',\n",
      "                                                                                                            'ScalingType': 'Auto'}]},\n",
      "                                                            'RandomSeed': 200,\n",
      "                                                            'ResourceLimits': {'MaxNumberOfTrainingJobs': 100,\n",
      "                                                                               'MaxParallelTrainingJobs': 2},\n",
      "                                                            'Strategy': 'Bayesian',\n",
      "                                                            'TrainingJobEarlyStoppingType': 'Auto'},\n",
      "                          'TrainingJobDefinition': {'AlgorithmSpecification': {'MetricDefinitions': [{'Name': 'validation:rmse',\n",
      "                                                                                                      'Regex': '.*validation:rmse '\n",
      "                                                                                                               '([0-9\\\\.]+).*'},\n",
      "                                                                                                     {'Name': 'train:rmse',\n",
      "                                                                                                      'Regex': '.*train:rmse '\n",
      "                                                                                                               '([0-9\\\\.]+).*'}],\n",
      "                                                                               'TrainingImage': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:1.2-1-cpu-py3',\n",
      "                                                                               'TrainingInputMode': 'File'},\n",
      "                                                    'HyperParameterTuningResourceConfig': {'InstanceCount': 1,\n",
      "                                                                                           'InstanceType': 'ml.m4.xlarge',\n",
      "                                                                                           'VolumeSizeInGB': 30},\n",
      "                                                    'InputDataConfig': [{'ChannelName': 'train',\n",
      "                                                                         'ContentType': 'text/csv',\n",
      "                                                                         'DataSource': {'S3DataSource': {'S3DataDistributionType': 'FullyReplicated',\n",
      "                                                                                                         'S3DataType': 'S3Prefix',\n",
      "                                                                                                         'S3Uri': {'Get': \"Steps.AbsaZindiProcess.ProcessingOutputConfig.Outputs['train'].S3Output.S3Uri\"}}}},\n",
      "                                                                        {'ChannelName': 'validation',\n",
      "                                                                         'ContentType': 'text/csv',\n",
      "                                                                         'DataSource': {'S3DataSource': {'S3DataDistributionType': 'FullyReplicated',\n",
      "                                                                                                         'S3DataType': 'S3Prefix',\n",
      "                                                                                                         'S3Uri': {'Get': \"Steps.AbsaZindiProcess.ProcessingOutputConfig.Outputs['validation'].S3Output.S3Uri\"}}}}],\n",
      "                                                    'OutputDataConfig': {'S3OutputPath': 's3://sagemaker-us-east-1-770208914484/absa_zindi_challenge/AbsaZindiTrain'},\n",
      "                                                    'RoleArn': 'arn:aws:iam::770208914484:role/service-role/AmazonSageMaker-ExecutionRole-20241229T183387',\n",
      "                                                    'StaticHyperParameters': {'sagemaker_container_log_level': '20',\n",
      "                                                                              'sagemaker_estimator_class_name': '\"SKLearn\"',\n",
      "                                                                              'sagemaker_estimator_module': '\"sagemaker.sklearn.estimator\"',\n",
      "                                                                              'sagemaker_job_name': '\"absa_zindi_challenge-training-2025-02-04-13-22-09-047\"',\n",
      "                                                                              'sagemaker_program': '\"train.py\"',\n",
      "                                                                              'sagemaker_region': '\"us-east-1\"',\n",
      "                                                                              'sagemaker_submit_directory': '\"s3://sagemaker-us-east-1-770208914484/absa_zindi_challenge-training-2025-02-04-13-22-09-047/source/sourcedir.tar.gz\"'},\n",
      "                                                    'StoppingCondition': {'MaxRuntimeInSeconds': 86400}}},\n",
      "            'Name': 'AbsaZindiHPTuning',\n",
      "            'Type': 'Tuning'},\n",
      "           {'Arguments': {'AppSpecification': {'ContainerEntrypoint': ['python3',\n",
      "                                                                       '/opt/ml/processing/input/code/evaluation.py'],\n",
      "                                               'ImageUri': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:1.2-1-cpu-py3'},\n",
      "                          'ProcessingInputs': [{'AppManaged': False,\n",
      "                                                'InputName': 'input-1',\n",
      "                                                'S3Input': {'LocalPath': '/opt/ml/processing/model',\n",
      "                                                            'S3CompressionType': 'None',\n",
      "                                                            'S3DataDistributionType': 'FullyReplicated',\n",
      "                                                            'S3DataType': 'S3Prefix',\n",
      "                                                            'S3InputMode': 'File',\n",
      "                                                            'S3Uri': {'Std:Join': {'On': '/',\n",
      "                                                                                   'Values': ['s3:/',\n",
      "                                                                                              'sagemaker-us-east-1-770208914484',\n",
      "                                                                                              'absa_zindi_challenge/AbsaZindiTrain',\n",
      "                                                                                              {'Get': 'Steps.AbsaZindiHPTuning.TrainingJobSummaries[0].TrainingJobName'},\n",
      "                                                                                              'output/model.tar.gz']}}}},\n",
      "                                               {'AppManaged': False,\n",
      "                                                'InputName': 'input-2',\n",
      "                                                'S3Input': {'LocalPath': '/opt/ml/processing/test',\n",
      "                                                            'S3CompressionType': 'None',\n",
      "                                                            'S3DataDistributionType': 'FullyReplicated',\n",
      "                                                            'S3DataType': 'S3Prefix',\n",
      "                                                            'S3InputMode': 'File',\n",
      "                                                            'S3Uri': {'Get': \"Steps.AbsaZindiProcess.ProcessingOutputConfig.Outputs['test'].S3Output.S3Uri\"}}},\n",
      "                                               {'AppManaged': False,\n",
      "                                                'InputName': 'code',\n",
      "                                                'S3Input': {'LocalPath': '/opt/ml/processing/input/code',\n",
      "                                                            'S3CompressionType': 'None',\n",
      "                                                            'S3DataDistributionType': 'FullyReplicated',\n",
      "                                                            'S3DataType': 'S3Prefix',\n",
      "                                                            'S3InputMode': 'File',\n",
      "                                                            'S3Uri': 's3://sagemaker-us-east-1-770208914484/AbsaZindiPipeline/code/46c97063f8bb77706b373e30fae68acb/evaluation.py'}}],\n",
      "                          'ProcessingOutputConfig': {'Outputs': [{'AppManaged': False,\n",
      "                                                                  'OutputName': 'evaluation',\n",
      "                                                                  'S3Output': {'LocalPath': '/opt/ml/processing/evaluation',\n",
      "                                                                               'S3UploadMode': 'EndOfJob',\n",
      "                                                                               'S3Uri': 's3://sagemaker-us-east-1-770208914484/sklearn-absa-zindi-evaluation-2025-02-04-13-22-07-677/output/evaluation'}}]},\n",
      "                          'ProcessingResources': {'ClusterConfig': {'InstanceCount': {'Get': 'Parameters.ProcessingInstanceCount'},\n",
      "                                                                    'InstanceType': 'ml.t3.large',\n",
      "                                                                    'VolumeSizeInGB': 30}},\n",
      "                          'RoleArn': 'arn:aws:iam::770208914484:role/service-role/AmazonSageMaker-ExecutionRole-20241229T183387'},\n",
      "            'CacheConfig': {'Enabled': True, 'ExpireAfter': '30d'},\n",
      "            'Name': 'AbsaZindiEvaluateTopModel',\n",
      "            'PropertyFiles': [{'FilePath': 'evaluation.json',\n",
      "                               'OutputName': 'evaluation',\n",
      "                               'PropertyFileName': 'BestTuningModelEvaluationReport'}],\n",
      "            'Type': 'Processing'},\n",
      "           {'Arguments': {'Conditions': [{'LeftValue': {'Std:JsonGet': {'Path': 'regression_metrics.rmse.value',\n",
      "                                                                        'PropertyFile': {'Get': 'Steps.AbsaZindiEvaluateTopModel.PropertyFiles.BestTuningModelEvaluationReport'}}},\n",
      "                                          'RightValue': {'Get': 'Parameters.RMSEThreshold'},\n",
      "                                          'Type': 'LessThanOrEqualTo'}],\n",
      "                          'ElseSteps': [{'Arguments': {'ErrorMessage': {'Std:Join': {'On': ' ',\n",
      "                                                                                     'Values': ['Execution '\n",
      "                                                                                                'failed '\n",
      "                                                                                                'due '\n",
      "                                                                                                'to '\n",
      "                                                                                                'root '\n",
      "                                                                                                'mean '\n",
      "                                                                                                'squared '\n",
      "                                                                                                'error '\n",
      "                                                                                                '>',\n",
      "                                                                                                {'Get': 'Parameters.RMSEThreshold'}]}}},\n",
      "                                         'Name': 'AbsaZindiRMSEScoreFail',\n",
      "                                         'Type': 'Fail'}],\n",
      "                          'IfSteps': [{'Arguments': {'InferenceSpecification': {'Containers': [{'Environment': {'SAGEMAKER_CONTAINER_LOG_LEVEL': '20',\n",
      "                                                                                                                'SAGEMAKER_PROGRAM': 'inference.py',\n",
      "                                                                                                                'SAGEMAKER_REGION': 'us-east-1',\n",
      "                                                                                                                'SAGEMAKER_SUBMIT_DIRECTORY': 's3://sagemaker-us-east-1-770208914484/sagemaker-scikit-learn-2025-02-04-13-22-08-235/sourcedir.tar.gz'},\n",
      "                                                                                                'Framework': 'SKLEARN',\n",
      "                                                                                                'Image': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:1.2-1-cpu-py3',\n",
      "                                                                                                'ModelDataUrl': {'Std:Join': {'On': '/',\n",
      "                                                                                                                              'Values': ['s3:/',\n",
      "                                                                                                                                         'sagemaker-us-east-1-770208914484',\n",
      "                                                                                                                                         'absa_zindi_challenge/AbsaZindiTrain',\n",
      "                                                                                                                                         {'Get': 'Steps.AbsaZindiHPTuning.TrainingJobSummaries[0].TrainingJobName'},\n",
      "                                                                                                                                         'output/model.tar.gz']}}}],\n",
      "                                                                                'SupportedContentTypes': ['text/csv'],\n",
      "                                                                                'SupportedRealtimeInferenceInstanceTypes': ['ml.t2.medium',\n",
      "                                                                                                                            'ml.m5.large'],\n",
      "                                                                                'SupportedResponseMIMETypes': ['text/csv'],\n",
      "                                                                                'SupportedTransformInstanceTypes': ['ml.m5.large']},\n",
      "                                                     'ModelApprovalStatus': {'Get': 'Parameters.ModelApprovalStatus'},\n",
      "                                                     'ModelPackageGroupName': 'absa-zindi-model-group',\n",
      "                                                     'SkipModelValidation': 'None'},\n",
      "                                       'Name': 'AbsaZindiRegisterBestModel-RegisterModel',\n",
      "                                       'Type': 'RegisterModel'},\n",
      "                                      {'Arguments': {'ExecutionRoleArn': 'arn:aws:iam::770208914484:role/service-role/AmazonSageMaker-ExecutionRole-20241229T183387',\n",
      "                                                     'PrimaryContainer': {'Environment': {'SAGEMAKER_CONTAINER_LOG_LEVEL': '20',\n",
      "                                                                                          'SAGEMAKER_PROGRAM': 'inference.py',\n",
      "                                                                                          'SAGEMAKER_REGION': 'us-east-1',\n",
      "                                                                                          'SAGEMAKER_SUBMIT_DIRECTORY': 's3://sagemaker-us-east-1-770208914484/sagemaker-scikit-learn-2025-02-04-13-22-08-068/sourcedir.tar.gz'},\n",
      "                                                                          'Image': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:1.2-1-cpu-py3',\n",
      "                                                                          'ModelDataUrl': {'Std:Join': {'On': '/',\n",
      "                                                                                                        'Values': ['s3:/',\n",
      "                                                                                                                   'sagemaker-us-east-1-770208914484',\n",
      "                                                                                                                   'absa_zindi_challenge/AbsaZindiTrain',\n",
      "                                                                                                                   {'Get': 'Steps.AbsaZindiHPTuning.TrainingJobSummaries[0].TrainingJobName'},\n",
      "                                                                                                                   'output/model.tar.gz']}}}},\n",
      "                                       'Name': 'AbsaZindiCreateBestModel-CreateModel',\n",
      "                                       'Type': 'Model'},\n",
      "                                      {'Arguments': {'ModelName': {'Get': 'Steps.AbsaZindiCreateBestModel-CreateModel.ModelName'},\n",
      "                                                     'TransformInput': {'ContentType': 'text/csv',\n",
      "                                                                        'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
      "                                                                                                        'S3Uri': {'Get': \"Steps.AbsaZindiProcess.ProcessingOutputConfig.Outputs['to_be_submitted'].S3Output.S3Uri\"}}},\n",
      "                                                                        'SplitType': 'Line'},\n",
      "                                                     'TransformOutput': {'S3OutputPath': 's3://sagemaker-us-east-1-770208914484/AbsaZindiTransform'},\n",
      "                                                     'TransformResources': {'InstanceCount': 1,\n",
      "                                                                            'InstanceType': 'ml.m4.xlarge'}},\n",
      "                                       'Name': 'AbsaZindiTransform',\n",
      "                                       'Type': 'Transform'}]},\n",
      "            'Name': 'AbsaZindiCheckRMSE',\n",
      "            'Type': 'Condition'}],\n",
      " 'Version': '2020-12-01'}\n"
     ]
    }
   ],
   "source": [
    "pipeline_name = f\"AbsaZindiPipeline\"\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_count,\n",
    "        instance_type,\n",
    "        model_approval_status,\n",
    "        train_data, \n",
    "        test_data, \n",
    "        customer_data,\n",
    "        transactions_data,\n",
    "        rmse_score_threshold\n",
    "    ],\n",
    "    steps=[step_process,\n",
    "           step_tuning, \n",
    "           step_eval, \n",
    "           step_cond\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "definition = json.loads(pipeline.definition())\n",
    "pprint(definition)\n",
    "pipeline.upsert(role_arn=role)\n",
    "execution = pipeline.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db17ac88-d529-4229-b59d-0355b207c47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CreatedBy': {'DomainId': 'd-a8oefxc7izan',\n",
      "               'IamIdentity': {'Arn': 'arn:aws:sts::770208914484:assumed-role/AmazonSageMaker-ExecutionRole-20241229T183387/SageMaker',\n",
      "                               'PrincipalId': 'AROA3GVAHLQ2AVBPXIJBW:SageMaker'},\n",
      "               'UserProfileArn': 'arn:aws:sagemaker:us-east-1:770208914484:user-profile/d-a8oefxc7izan/hlulani-mlops',\n",
      "               'UserProfileName': 'hlulani-mlops'},\n",
      " 'CreationTime': datetime.datetime(2025, 2, 4, 13, 22, 10, 663000, tzinfo=tzlocal()),\n",
      " 'LastModifiedBy': {'DomainId': 'd-a8oefxc7izan',\n",
      "                    'IamIdentity': {'Arn': 'arn:aws:sts::770208914484:assumed-role/AmazonSageMaker-ExecutionRole-20241229T183387/SageMaker',\n",
      "                                    'PrincipalId': 'AROA3GVAHLQ2AVBPXIJBW:SageMaker'},\n",
      "                    'UserProfileArn': 'arn:aws:sagemaker:us-east-1:770208914484:user-profile/d-a8oefxc7izan/hlulani-mlops',\n",
      "                    'UserProfileName': 'hlulani-mlops'},\n",
      " 'LastModifiedTime': datetime.datetime(2025, 2, 4, 13, 22, 10, 663000, tzinfo=tzlocal()),\n",
      " 'PipelineArn': 'arn:aws:sagemaker:us-east-1:770208914484:pipeline/AbsaZindiPipeline',\n",
      " 'PipelineExecutionArn': 'arn:aws:sagemaker:us-east-1:770208914484:pipeline/AbsaZindiPipeline/execution/vwrkyuv747yu',\n",
      " 'PipelineExecutionDisplayName': 'execution-1738675330719',\n",
      " 'PipelineExecutionStatus': 'Executing',\n",
      " 'PipelineExperimentConfig': {'ExperimentName': 'absazindipipeline',\n",
      "                              'TrialName': 'vwrkyuv747yu'},\n",
      " 'ResponseMetadata': {'HTTPHeaders': {'content-length': '1152',\n",
      "                                      'content-type': 'application/x-amz-json-1.1',\n",
      "                                      'date': 'Tue, 04 Feb 2025 14:10:15 GMT',\n",
      "                                      'x-amzn-requestid': '44683556-9424-42c2-88d8-d25bea43ba13'},\n",
      "                      'HTTPStatusCode': 200,\n",
      "                      'RequestId': '44683556-9424-42c2-88d8-d25bea43ba13',\n",
      "                      'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "pprint(execution.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcd2719-cfff-49fe-a1fd-139e13898121",
   "metadata": {},
   "source": [
    "### Code Explanation: Describing and Listing Pipeline Execution Steps\n",
    "  \n",
    "#### `execution.list_steps()`:\n",
    "   - This method returns the list of steps that have been executed or are pending in the pipeline.\n",
    "   - **Purpose**: It gives insight into the individual steps of the pipeline, showing whether they have been completed, are in progress, or are yet to be started.\n",
    "   - **Use Case**: It's useful for tracking the execution status of each step within the pipeline. For example, you can use it to confirm whether steps like data processing, hyperparameter tuning, evaluation, and model registration are successfully completed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c2d0177-eb07-4622-b923-7d9c510becf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'AttemptCount': 1,\n",
      "  'Metadata': {'TransformJob': {'Arn': 'arn:aws:sagemaker:us-east-1:770208914484:transform-job/pipelines-vwrkyuv747yu-AbsaZindiTransform-a4fFd1nxwz'}},\n",
      "  'StartTime': datetime.datetime(2025, 2, 4, 15, 27, 49, 707000, tzinfo=tzlocal()),\n",
      "  'StepName': 'AbsaZindiTransform',\n",
      "  'StepStatus': 'Executing'},\n",
      " {'AttemptCount': 1,\n",
      "  'EndTime': datetime.datetime(2025, 2, 4, 15, 27, 49, 268000, tzinfo=tzlocal()),\n",
      "  'Metadata': {'Model': {'Arn': 'arn:aws:sagemaker:us-east-1:770208914484:model/pipelines-vwrkyuv747yu-AbsaZindiCreateBestM-bAtYYYUAi6'}},\n",
      "  'StartTime': datetime.datetime(2025, 2, 4, 15, 27, 47, 239000, tzinfo=tzlocal()),\n",
      "  'StepName': 'AbsaZindiCreateBestModel-CreateModel',\n",
      "  'StepStatus': 'Succeeded'},\n",
      " {'AttemptCount': 1,\n",
      "  'EndTime': datetime.datetime(2025, 2, 4, 15, 27, 49, 59000, tzinfo=tzlocal()),\n",
      "  'Metadata': {'RegisterModel': {'Arn': 'arn:aws:sagemaker:us-east-1:770208914484:model-package/absa-zindi-model-group/11'}},\n",
      "  'StartTime': datetime.datetime(2025, 2, 4, 15, 27, 47, 239000, tzinfo=tzlocal()),\n",
      "  'StepName': 'AbsaZindiRegisterBestModel-RegisterModel',\n",
      "  'StepStatus': 'Succeeded'},\n",
      " {'AttemptCount': 1,\n",
      "  'EndTime': datetime.datetime(2025, 2, 4, 15, 27, 46, 405000, tzinfo=tzlocal()),\n",
      "  'Metadata': {'Condition': {'Outcome': 'True'}},\n",
      "  'StartTime': datetime.datetime(2025, 2, 4, 15, 27, 46, 232000, tzinfo=tzlocal()),\n",
      "  'StepName': 'AbsaZindiCheckRMSE',\n",
      "  'StepStatus': 'Succeeded'},\n",
      " {'AttemptCount': 1,\n",
      "  'EndTime': datetime.datetime(2025, 2, 4, 15, 27, 45, 933000, tzinfo=tzlocal()),\n",
      "  'Metadata': {'ProcessingJob': {'Arn': 'arn:aws:sagemaker:us-east-1:770208914484:processing-job/pipelines-vwrkyuv747yu-AbsaZindiEvaluateTop-nEYdgAvJHl'}},\n",
      "  'StartTime': datetime.datetime(2025, 2, 4, 15, 22, 40, 938000, tzinfo=tzlocal()),\n",
      "  'StepName': 'AbsaZindiEvaluateTopModel',\n",
      "  'StepStatus': 'Succeeded'},\n",
      " {'AttemptCount': 1,\n",
      "  'EndTime': datetime.datetime(2025, 2, 4, 15, 22, 40, 508000, tzinfo=tzlocal()),\n",
      "  'Metadata': {'TuningJob': {'Arn': 'arn:aws:sagemaker:us-east-1:770208914484:hyper-parameter-tuning-job/vwrkyuv747yu-AbsaZind-SCWhTjgPx3'}},\n",
      "  'StartTime': datetime.datetime(2025, 2, 4, 14, 22, 23, 467000, tzinfo=tzlocal()),\n",
      "  'StepName': 'AbsaZindiHPTuning',\n",
      "  'StepStatus': 'Succeeded'},\n",
      " {'AttemptCount': 1,\n",
      "  'EndTime': datetime.datetime(2025, 2, 4, 14, 22, 23, tzinfo=tzlocal()),\n",
      "  'Metadata': {'ProcessingJob': {'Arn': 'arn:aws:sagemaker:us-east-1:770208914484:processing-job/pipelines-vwrkyuv747yu-AbsaZindiProcess-OpVXMdoG6t'}},\n",
      "  'StartTime': datetime.datetime(2025, 2, 4, 13, 22, 12, 163000, tzinfo=tzlocal()),\n",
      "  'StepName': 'AbsaZindiProcess',\n",
      "  'StepStatus': 'Succeeded'}]\n"
     ]
    }
   ],
   "source": [
    "pprint(execution.list_steps())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5ad2b7-e338-479b-b49c-5d3dc881bc40",
   "metadata": {},
   "source": [
    "### Print Evaluation Metrics\n",
    "\n",
    " **`sagemaker.s3.S3Downloader.read_file()`**:\n",
    "   - This method is used to read a file from an S3 bucket. Specifically, it downloads the `evaluation.json` file from the S3 location where the model evaluation results are stored.\n",
    "   - **Parameters**:\n",
    "     - The S3 path is dynamically constructed using the URI from the `step_eval` step's output configuration.\n",
    "     - The `evaluation.json` file contains the model evaluation metrics, such as RMSE values.\n",
    "\n",
    "   - **Purpose**: This function helps retrieve the evaluation results for further analysis or display.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3fc0e10e-7b46-4f16-985b-194bf617ffbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'regression_metrics': {'rmse': {'standard_deviation': 6079.155867874953,\n",
      "                                 'value': 6088.532246391974}}}\n"
     ]
    }
   ],
   "source": [
    "evaluation_json = sagemaker.s3.S3Downloader.read_file(\n",
    "    \"{}/evaluation.json\".format(\n",
    "        step_eval.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n",
    "    )\n",
    ")\n",
    "pprint(json.loads(evaluation_json))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf6d6b5-62dc-49c2-b208-65671f003766",
   "metadata": {},
   "source": [
    "# Lineage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2b2fb9f-aad8-4ef1-956e-2e9de7b2503a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'StepName': 'AbsaZindiProcess', 'StartTime': datetime.datetime(2025, 2, 4, 13, 22, 12, 163000, tzinfo=tzlocal()), 'EndTime': datetime.datetime(2025, 2, 4, 14, 22, 23, tzinfo=tzlocal()), 'StepStatus': 'Succeeded', 'Metadata': {'ProcessingJob': {'Arn': 'arn:aws:sagemaker:us-east-1:770208914484:processing-job/pipelines-vwrkyuv747yu-AbsaZindiProcess-OpVXMdoG6t'}}, 'AttemptCount': 1}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name/Source</th>\n",
       "      <th>Direction</th>\n",
       "      <th>Type</th>\n",
       "      <th>Association Type</th>\n",
       "      <th>Lineage Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s3://...a078ad6cf0714fc4fe9be41/preprocessing.py</td>\n",
       "      <td>Input</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s3://...allenge/input-data/customer/customer.csv</td>\n",
       "      <td>Input</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s3://...input-data/transactions/transactions.csv</td>\n",
       "      <td>Input</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s3://...zindi_challenge/input-data/test/Test.csv</td>\n",
       "      <td>Input</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s3://...ndi_challenge/input-data/train/Train.csv</td>\n",
       "      <td>Input</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>68331...com/sagemaker-scikit-learn:1.2-1-cpu-py3</td>\n",
       "      <td>Input</td>\n",
       "      <td>Image</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>s3://...lenge/opt/ml/processing/output/artifacts</td>\n",
       "      <td>Output</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>Produced</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>s3://...opt/ml/processing/output/to_be_submitted</td>\n",
       "      <td>Output</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>Produced</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>s3://..._challenge/opt/ml/processing/output/test</td>\n",
       "      <td>Output</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>Produced</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>s3://...enge/opt/ml/processing/output/validation</td>\n",
       "      <td>Output</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>Produced</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>s3://...challenge/opt/ml/processing/output/train</td>\n",
       "      <td>Output</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>Produced</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Name/Source Direction     Type  \\\n",
       "0   s3://...a078ad6cf0714fc4fe9be41/preprocessing.py     Input  DataSet   \n",
       "1   s3://...allenge/input-data/customer/customer.csv     Input  DataSet   \n",
       "2   s3://...input-data/transactions/transactions.csv     Input  DataSet   \n",
       "3   s3://...zindi_challenge/input-data/test/Test.csv     Input  DataSet   \n",
       "4   s3://...ndi_challenge/input-data/train/Train.csv     Input  DataSet   \n",
       "5   68331...com/sagemaker-scikit-learn:1.2-1-cpu-py3     Input    Image   \n",
       "6   s3://...lenge/opt/ml/processing/output/artifacts    Output  DataSet   \n",
       "7   s3://...opt/ml/processing/output/to_be_submitted    Output  DataSet   \n",
       "8   s3://..._challenge/opt/ml/processing/output/test    Output  DataSet   \n",
       "9   s3://...enge/opt/ml/processing/output/validation    Output  DataSet   \n",
       "10  s3://...challenge/opt/ml/processing/output/train    Output  DataSet   \n",
       "\n",
       "   Association Type Lineage Type  \n",
       "0     ContributedTo     artifact  \n",
       "1     ContributedTo     artifact  \n",
       "2     ContributedTo     artifact  \n",
       "3     ContributedTo     artifact  \n",
       "4     ContributedTo     artifact  \n",
       "5     ContributedTo     artifact  \n",
       "6          Produced     artifact  \n",
       "7          Produced     artifact  \n",
       "8          Produced     artifact  \n",
       "9          Produced     artifact  \n",
       "10         Produced     artifact  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'StepName': 'AbsaZindiHPTuning', 'StartTime': datetime.datetime(2025, 2, 4, 14, 22, 23, 467000, tzinfo=tzlocal()), 'EndTime': datetime.datetime(2025, 2, 4, 15, 22, 40, 508000, tzinfo=tzlocal()), 'StepStatus': 'Succeeded', 'Metadata': {'TuningJob': {'Arn': 'arn:aws:sagemaker:us-east-1:770208914484:hyper-parameter-tuning-job/vwrkyuv747yu-AbsaZind-SCWhTjgPx3'}}, 'AttemptCount': 1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'StepName': 'AbsaZindiEvaluateTopModel', 'StartTime': datetime.datetime(2025, 2, 4, 15, 22, 40, 938000, tzinfo=tzlocal()), 'EndTime': datetime.datetime(2025, 2, 4, 15, 27, 45, 933000, tzinfo=tzlocal()), 'StepStatus': 'Succeeded', 'Metadata': {'ProcessingJob': {'Arn': 'arn:aws:sagemaker:us-east-1:770208914484:processing-job/pipelines-vwrkyuv747yu-AbsaZindiEvaluateTop-nEYdgAvJHl'}}, 'AttemptCount': 1}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name/Source</th>\n",
       "      <th>Direction</th>\n",
       "      <th>Type</th>\n",
       "      <th>Association Type</th>\n",
       "      <th>Lineage Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s3://...63f8bb77706b373e30fae68acb/evaluation.py</td>\n",
       "      <td>Input</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s3://..._challenge/opt/ml/processing/output/test</td>\n",
       "      <td>Input</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s3://...hTjgPx3-087-343e21e1/output/model.tar.gz</td>\n",
       "      <td>Input</td>\n",
       "      <td>Model</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>68331...com/sagemaker-scikit-learn:1.2-1-cpu-py3</td>\n",
       "      <td>Input</td>\n",
       "      <td>Image</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s3://...025-02-04-13-22-07-677/output/evaluation</td>\n",
       "      <td>Output</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>Produced</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Name/Source Direction     Type  \\\n",
       "0  s3://...63f8bb77706b373e30fae68acb/evaluation.py     Input  DataSet   \n",
       "1  s3://..._challenge/opt/ml/processing/output/test     Input  DataSet   \n",
       "2  s3://...hTjgPx3-087-343e21e1/output/model.tar.gz     Input    Model   \n",
       "3  68331...com/sagemaker-scikit-learn:1.2-1-cpu-py3     Input    Image   \n",
       "4  s3://...025-02-04-13-22-07-677/output/evaluation    Output  DataSet   \n",
       "\n",
       "  Association Type Lineage Type  \n",
       "0    ContributedTo     artifact  \n",
       "1    ContributedTo     artifact  \n",
       "2    ContributedTo     artifact  \n",
       "3    ContributedTo     artifact  \n",
       "4         Produced     artifact  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'StepName': 'AbsaZindiCheckRMSE', 'StartTime': datetime.datetime(2025, 2, 4, 15, 27, 46, 232000, tzinfo=tzlocal()), 'EndTime': datetime.datetime(2025, 2, 4, 15, 27, 46, 405000, tzinfo=tzlocal()), 'StepStatus': 'Succeeded', 'Metadata': {'Condition': {'Outcome': 'True'}}, 'AttemptCount': 1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'StepName': 'AbsaZindiRegisterBestModel-RegisterModel', 'StartTime': datetime.datetime(2025, 2, 4, 15, 27, 47, 239000, tzinfo=tzlocal()), 'EndTime': datetime.datetime(2025, 2, 4, 15, 27, 49, 59000, tzinfo=tzlocal()), 'StepStatus': 'Succeeded', 'Metadata': {'RegisterModel': {'Arn': 'arn:aws:sagemaker:us-east-1:770208914484:model-package/absa-zindi-model-group/11'}}, 'AttemptCount': 1}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name/Source</th>\n",
       "      <th>Direction</th>\n",
       "      <th>Type</th>\n",
       "      <th>Association Type</th>\n",
       "      <th>Lineage Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s3://...hTjgPx3-087-343e21e1/output/model.tar.gz</td>\n",
       "      <td>Input</td>\n",
       "      <td>Model</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>68331...com/sagemaker-scikit-learn:1.2-1-cpu-py3</td>\n",
       "      <td>Input</td>\n",
       "      <td>Image</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>absa-zindi-model-group-11-PendingManualApprova...</td>\n",
       "      <td>Input</td>\n",
       "      <td>Approval</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>absa-zindi-model-group-1738475303-aws-model-pa...</td>\n",
       "      <td>Output</td>\n",
       "      <td>ModelGroup</td>\n",
       "      <td>AssociatedWith</td>\n",
       "      <td>context</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Name/Source Direction        Type  \\\n",
       "0   s3://...hTjgPx3-087-343e21e1/output/model.tar.gz     Input       Model   \n",
       "1   68331...com/sagemaker-scikit-learn:1.2-1-cpu-py3     Input       Image   \n",
       "2  absa-zindi-model-group-11-PendingManualApprova...     Input    Approval   \n",
       "3  absa-zindi-model-group-1738475303-aws-model-pa...    Output  ModelGroup   \n",
       "\n",
       "  Association Type Lineage Type  \n",
       "0    ContributedTo     artifact  \n",
       "1    ContributedTo     artifact  \n",
       "2    ContributedTo       action  \n",
       "3   AssociatedWith      context  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'StepName': 'AbsaZindiCreateBestModel-CreateModel', 'StartTime': datetime.datetime(2025, 2, 4, 15, 27, 47, 239000, tzinfo=tzlocal()), 'EndTime': datetime.datetime(2025, 2, 4, 15, 27, 49, 268000, tzinfo=tzlocal()), 'StepStatus': 'Succeeded', 'Metadata': {'Model': {'Arn': 'arn:aws:sagemaker:us-east-1:770208914484:model/pipelines-vwrkyuv747yu-AbsaZindiCreateBestM-bAtYYYUAi6'}}, 'AttemptCount': 1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'StepName': 'AbsaZindiTransform', 'StartTime': datetime.datetime(2025, 2, 4, 15, 27, 49, 707000, tzinfo=tzlocal()), 'EndTime': datetime.datetime(2025, 2, 4, 15, 34, 7, 229000, tzinfo=tzlocal()), 'StepStatus': 'Succeeded', 'Metadata': {'TransformJob': {'Arn': 'arn:aws:sagemaker:us-east-1:770208914484:transform-job/pipelines-vwrkyuv747yu-AbsaZindiTransform-a4fFd1nxwz'}}, 'AttemptCount': 1}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name/Source</th>\n",
       "      <th>Direction</th>\n",
       "      <th>Type</th>\n",
       "      <th>Association Type</th>\n",
       "      <th>Lineage Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s3://...hTjgPx3-087-343e21e1/output/model.tar.gz</td>\n",
       "      <td>Input</td>\n",
       "      <td>Model</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>68331...com/sagemaker-scikit-learn:1.2-1-cpu-py3</td>\n",
       "      <td>Input</td>\n",
       "      <td>Image</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s3://...opt/ml/processing/output/to_be_submitted</td>\n",
       "      <td>Input</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s3://...s-east-1-770208914484/AbsaZindiTransform</td>\n",
       "      <td>Output</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>Produced</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Name/Source Direction     Type  \\\n",
       "0  s3://...hTjgPx3-087-343e21e1/output/model.tar.gz     Input    Model   \n",
       "1  68331...com/sagemaker-scikit-learn:1.2-1-cpu-py3     Input    Image   \n",
       "2  s3://...opt/ml/processing/output/to_be_submitted     Input  DataSet   \n",
       "3  s3://...s-east-1-770208914484/AbsaZindiTransform    Output  DataSet   \n",
       "\n",
       "  Association Type Lineage Type  \n",
       "0    ContributedTo     artifact  \n",
       "1    ContributedTo     artifact  \n",
       "2    ContributedTo     artifact  \n",
       "3         Produced     artifact  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "from sagemaker.lineage.visualizer import LineageTableVisualizer\n",
    "\n",
    "\n",
    "viz = LineageTableVisualizer(sagemaker.session.Session())\n",
    "for execution_step in reversed(execution.list_steps()):\n",
    "    print(execution_step)\n",
    "    display(viz.show(pipeline_execution_step=execution_step))\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb938fa5-f1c0-401c-a982-272281652215",
   "metadata": {},
   "source": [
    "# Useful resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2372e74-4988-41a1-82c7-c3fc2ef98680",
   "metadata": {},
   "source": [
    "https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker-pipelines/tabular/tuning-step/sagemaker-pipelines-tuning-step.ipynb\n",
    "\n",
    "https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-python-sdk/scikit_learn_randomforest/Sklearn_on_SageMaker_end2end.html\n",
    "\n",
    "https://github.com/aws-samples/mlops-pipeline-prestodb/blob/main/0_model_training_pipeline.ipynb\n",
    "\n",
    "https://github.com/aws/amazon-sagemaker-examples/issues/1207\n",
    "\n",
    "https://github.com/aws/sagemaker-python-sdk/issues/3932\n",
    "\n",
    "https://gmoein.github.io/files/Amazon%20SageMaker.pdf\n",
    "\n",
    "https://github.com/aws/amazon-sagemaker-examples/issues/1920\n",
    "\n",
    "https://medium.com/@786sksujanislam786/bring-your-own-sklearn-algorithm-in-sagemaker-fe573784926e\n",
    "\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps-types.html\n",
    "\n",
    "https://github.com/aws/sagemaker-inference-toolkit/blob/master/README.md\n",
    "\n",
    "https://github.com/RamVegiraju/Pre-Trained-Sklearn-SageMaker/blob/master/main.py\n",
    "\n",
    "https://www.datacamp.com/tutorial/pyspark-tutorial-getting-started-with-pyspark\n",
    "\n",
    "https://sagemaker-examples.readthedocs.io/en/latest/sagemaker_processing/spark_distributed_data_processing/sagemaker-spark-processing.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
